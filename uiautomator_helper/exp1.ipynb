{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Extraction Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xml.dom import minidom\n",
    "from collections import defaultdict \n",
    "import numpy as np\n",
    "import editdistance\n",
    "import random\n",
    "\n",
    "EMBEDDING_DIM = 16\n",
    "\n",
    "MAXN_STATE_NODES = 400 # maximum number of state nodes used\n",
    "MAX_TOKEN_LENGTH = 20 # maximum token length padded to\n",
    "\n",
    "NODE_KEY_LIST = [ \n",
    "    # slot names (keys) of a node to add as features\n",
    "    \"index\", # integer\n",
    "    \"checkable\", \"checked\", \"clickable\", \"enabled\", \"focusable\", \"focused\", # boolean\n",
    "    \"scrollable\", \"long-clickable\", \"password\", \"selected\", \"visible-to-user\", # boolean\n",
    "    \"bounds\", # interval\n",
    "    \"content-desc\", # string\n",
    "    \"resource-id\", \"class\", \"package\", # formatted string\n",
    "]\n",
    "NODE_KEY_DICT = {NODE_KEY_LIST[i]:i for i in range(len(NODE_KEY_LIST))}\n",
    "\n",
    "CHAR_LIST = [\"<PAD>\", \"<UNK>\"] +\\\n",
    "list(\"ABCDEFGHIJKLMNOPQRSTUVWXYZ\") +\\\n",
    "list(\"abcdefghijklmnopqrstuvwxyz\") +\\\n",
    "list(\"0123456789\") +\\\n",
    "list(\"`~!@#$%^&*()_+-={}|[]:;'',.<>/?\") +\\\n",
    "[\"\\\\\"] + ['\"']\n",
    "CHAR_DICT = defaultdict(\n",
    "    lambda:CHAR_LIST.index(\"<UNK>\"), \n",
    "    {CHAR_LIST[i]:i for i in range(len(CHAR_LIST))}\n",
    ")\n",
    "\n",
    "PADDING_NODE_VECTOR = [ [CHAR_DICT[\"<PAD>\"] for _ in range(MAX_TOKEN_LENGTH)] for _ in range(len(NODE_KEY_LIST))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specific method for target encoding\n",
    "# similar to get_node_vector but without the node assumption\n",
    "# arg_node is a list of strings\n",
    "def get_sentence_vector(arg_sent):\n",
    "    sent_vector = []\n",
    "    for j in range(len(arg_sent)):\n",
    "        chars_j = list(arg_sent[j])\n",
    "        # get the indices for every char\n",
    "        inds_j = [CHAR_DICT[chars_j[k]] for k in range( min(MAX_TOKEN_LENGTH,len(chars_j)) )]\n",
    "        # pad the inds\n",
    "        inds_j += [CHAR_DICT[\"<PAD>\"]] * ( MAX_TOKEN_LENGTH-len(inds_j) )\n",
    "        sent_vector.append(inds_j)\n",
    "    return sent_vector\n",
    "\n",
    "# arg_node is a gui element object\n",
    "# (element from action list)\n",
    "def get_element_vector(arg_elem):\n",
    "    elem_vector = []\n",
    "    for j in range(len(NODE_KEY_LIST)):\n",
    "        key_j = NODE_KEY_LIST[j]\n",
    "        str_j = str(arg_elem.attributes[key_j])\n",
    "        chars_j = list(str_j)\n",
    "        # get the indices for every char\n",
    "        inds_j = [CHAR_DICT[chars_j[k]] for k in range( min(MAX_TOKEN_LENGTH,len(chars_j)) )]\n",
    "        # pad the inds\n",
    "        inds_j += [CHAR_DICT[\"<PAD>\"]] * ( MAX_TOKEN_LENGTH-len(inds_j) )\n",
    "        elem_vector.append(inds_j)\n",
    "    return elem_vector\n",
    "\n",
    "# arg_node is a ui element object\n",
    "# (node from state)\n",
    "def get_node_vector(arg_node):\n",
    "    node_vector = []\n",
    "    for j in range(len(NODE_KEY_LIST)):\n",
    "        key_j = NODE_KEY_LIST[j]\n",
    "        str_j = str(arg_node.attributes[key_j].value)\n",
    "        chars_j = list(str_j)\n",
    "        # get the indices for every char\n",
    "        inds_j = [CHAR_DICT[chars_j[k]] for k in range( min(MAX_TOKEN_LENGTH,len(chars_j)) )]\n",
    "        # pad the inds\n",
    "        inds_j += [CHAR_DICT[\"<PAD>\"]] * ( MAX_TOKEN_LENGTH-len(inds_j) )\n",
    "        node_vector.append(inds_j)\n",
    "    return node_vector\n",
    "\n",
    "# a state here is a windows hierarchy string\n",
    "def get_state_matrix(arg_wh):\n",
    "    state_nodes = minidom.parseString(arg_wh).getElementsByTagName('node')\n",
    "    state_matrix = []\n",
    "    for i in range( min(MAXN_STATE_NODES,len(state_nodes)) ):\n",
    "        state_vector = []\n",
    "        node_i = state_nodes[i]\n",
    "        node_vector_i = get_node_vector(node_i)\n",
    "        state_matrix.append(node_vector_i)\n",
    "    # pad the state matrix\n",
    "    state_matrix += [PADDING_NODE_VECTOR] * ( MAXN_STATE_NODES-len(state_matrix) )\n",
    "    return state_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Agent\n",
    "- UniversalTokenEncoder: relu\n",
    "- StateEncoder: relu\n",
    "- TargetEncoder: relu\n",
    "- ActionEncoder: sigmoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cpu\n",
      "torch version: 1.6.0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"device: {}\".format(device))\n",
    "print(\"torch version: {}\".format(torch.__version__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UniversalTokenEncoder(nn.Module):\n",
    "    def __init__(self, arg_embedding_dim):\n",
    "        super(UniversalTokenEncoder, self).__init__()\n",
    "        self.embedding_dim = arg_embedding_dim\n",
    "        self.char_embedding = nn.Embedding(\n",
    "            num_embeddings=len(CHAR_LIST),\n",
    "            embedding_dim=arg_embedding_dim,\n",
    "        )\n",
    "        self.n_kernels = [10, 10, 10]\n",
    "        self.kernel_sizes = [1, 2, 3]\n",
    "        self.convs = nn.ModuleList([\n",
    "            nn.Conv1d(\n",
    "                in_channels=arg_embedding_dim, \n",
    "                out_channels=self.n_kernels[i], \n",
    "                kernel_size=self.kernel_sizes[i], \n",
    "                padding=0\n",
    "            )\n",
    "            for i in range(len(self.kernel_sizes))\n",
    "        ])\n",
    "        self.pools = nn.ModuleList([\n",
    "            nn.MaxPool1d(MAX_TOKEN_LENGTH+1-self.kernel_sizes[i], padding=0)\n",
    "            for i in range(len(self.kernel_sizes))\n",
    "        ])\n",
    "        self.linear = nn.Linear(sum(self.n_kernels), arg_embedding_dim)\n",
    "        \n",
    "    # input a batch of sequences (high-dimensional)\n",
    "    # arg_seqs: (B=1, ??, len(NODE_KEY_LIST), MAX_TOKEN_LENGTH)\n",
    "    # ??=MAXN_STATE_NODES if encoding state matrix\n",
    "    # ??=1 if encoding the target\n",
    "    # ??=others if encoding an action list\n",
    "    def forward(self, arg_seqs):\n",
    "        B = arg_seqs.shape[0]\n",
    "        tmpn_nodes = arg_seqs.shape[1]\n",
    "        tmp_dim0 = B * tmpn_nodes * len(NODE_KEY_LIST)\n",
    "        assert B==1\n",
    "        # first fold the first 3 dimensions\n",
    "        tmp0 = arg_seqs.view(tmp_dim0, MAX_TOKEN_LENGTH) # (dim0, MAX_TOKEN_LENGTH)\n",
    "        tmp1 = self.char_embedding(tmp0) # (dim0, MAX_TOKEN_LENGTH, embedding_dim)\n",
    "        tmp2 = tmp1.transpose(1,2) # channel goes first for conv, (dim0, embedding_dim, MAX_TOKEN_LENGTH)\n",
    "        # (dim0, n_kernels, MAX_TOKEN_LENGTH-i)\n",
    "        tmp3s = [\n",
    "            F.relu(self.convs[i](tmp2))\n",
    "            for i in range(len(self.convs))\n",
    "        ]\n",
    "        # (dim0, n_kernels, 1)\n",
    "        tmp4s = [\n",
    "            self.pools[i](tmp3s[i])\n",
    "            for i in range(len(tmp3s))\n",
    "        ]\n",
    "        # (dim0, n_kernels)\n",
    "        tmp5s = [\n",
    "            tmp4s[i].view(tmp_dim0, self.n_kernels[i])\n",
    "            for i in range(len(tmp4s))\n",
    "        ]\n",
    "        tmp6 = torch.cat(tmp5s, 1) # (dim0, sum(n_kernels))\n",
    "        tmp7 = F.relu(self.linear(tmp6)) # (dim0, embedding_dim)\n",
    "        # unfold back to original shape\n",
    "        # which is (B=1, ??={MAXN_STATE_NODES,1,others}, len(NODE_KEY_LIST), embedding_dim)\n",
    "        tmp8 = tmp7.view(B, tmpn_nodes, len(NODE_KEY_LIST), self.embedding_dim)\n",
    "        return tmp8\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StateEncoder(nn.Module):\n",
    "    def __init__(self, arg_embedding_dim):\n",
    "        super(StateEncoder, self).__init__()\n",
    "        self.embedding_dim = arg_embedding_dim\n",
    "        self.n_kernels = [10, 10, 10]\n",
    "        self.kernel_sizes = [(1,1), (2,2), (3,3)]\n",
    "        self.convs = nn.ModuleList([\n",
    "            nn.Conv2d(\n",
    "                in_channels=arg_embedding_dim, \n",
    "                out_channels=self.n_kernels[i], \n",
    "                kernel_size=self.kernel_sizes[i], \n",
    "                padding=0\n",
    "            )\n",
    "            for i in range(len(self.kernel_sizes))\n",
    "        ])\n",
    "        self.pools = nn.ModuleList([\n",
    "            nn.MaxPool2d(\n",
    "                kernel_size=(\n",
    "                    MAXN_STATE_NODES+1-self.kernel_sizes[i][0],\n",
    "                    len(NODE_KEY_LIST)+1-self.kernel_sizes[i][1],\n",
    "                ), \n",
    "                padding=0\n",
    "            )\n",
    "            for i in range(len(self.kernel_sizes))\n",
    "        ])\n",
    "        self.linear = nn.Linear(sum(self.n_kernels), arg_embedding_dim)\n",
    "        \n",
    "    # input a batch of sequences (high-dimensional)\n",
    "    # arg_seqs: (B=1, ??, len(NODE_KEY_LIST), embedding_dim)\n",
    "    # ??=MAXN_STATE_NODES since it's encoding state matrix\n",
    "    def forward(self, arg_seqs):\n",
    "        B = arg_seqs.shape[0]\n",
    "        tmpn_nodes = arg_seqs.shape[1]\n",
    "        assert B==1\n",
    "        assert tmpn_nodes==MAXN_STATE_NODES\n",
    "        # permute for conv\n",
    "        tmp0 = arg_seqs.permute(0,3,1,2) # (B=1, embedding_dim, ??, len(NODE_KEY_LIST))\n",
    "        # (B=1, n_kernels, ??-i, len(NODE_KEY_LIST)-i)\n",
    "        tmp1s = [\n",
    "            F.relu(self.convs[i](tmp0))\n",
    "            for i in range(len(self.convs))\n",
    "        ]\n",
    "        # (B=1, n_kernels, 1, 1)\n",
    "        tmp2s = [\n",
    "            self.pools[i](tmp1s[i])\n",
    "            for i in range(len(tmp1s))\n",
    "        ]\n",
    "        # (B=1, n_kernels)\n",
    "        tmp3s = [\n",
    "            tmp2s[i].view(B, self.n_kernels[i])\n",
    "            for i in range(len(tmp2s))\n",
    "        ]\n",
    "        tmp4 = torch.cat(tmp3s, 1) # (B, sum(n_kernels))\n",
    "        tmp5 = F.relu(self.linear(tmp4)) # (B, embedding_dim)\n",
    "        return tmp5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TargetEncoder(nn.Module):\n",
    "    def __init__(self, arg_embedding_dim):\n",
    "        super(TargetEncoder, self).__init__()\n",
    "        self.embedding_dim = arg_embedding_dim\n",
    "        self.n_kernels = [10, 10, 10]\n",
    "        self.kernel_sizes = [1, 2, 3]\n",
    "        self.convs = nn.ModuleList([\n",
    "            nn.Conv1d(\n",
    "                in_channels=arg_embedding_dim, \n",
    "                out_channels=self.n_kernels[i], \n",
    "                kernel_size=self.kernel_sizes[i], \n",
    "                padding=0\n",
    "            )\n",
    "            for i in range(len(self.kernel_sizes))\n",
    "        ])\n",
    "        self.pools = nn.ModuleList([\n",
    "            nn.MaxPool1d(\n",
    "                kernel_size=(\n",
    "                    len(NODE_KEY_LIST)+1-self.kernel_sizes[i],\n",
    "                ), \n",
    "                padding=0\n",
    "            )\n",
    "            for i in range(len(self.kernel_sizes))\n",
    "        ])\n",
    "        self.linear = nn.Linear(sum(self.n_kernels), arg_embedding_dim)\n",
    "        \n",
    "    # input a batch of sequences (high-dimensional)\n",
    "    # arg_seqs: (B=1, ??, len(NODE_KEY_LIST), embedding_dim)\n",
    "    # ??=1 since it's encoding a target\n",
    "    def forward(self, arg_seqs):\n",
    "        B = arg_seqs.shape[0]\n",
    "        tmpn_nodes = arg_seqs.shape[1]\n",
    "        tmp_dim0 = B * tmpn_nodes\n",
    "        assert B==1\n",
    "        assert tmpn_nodes==1\n",
    "        # action nodes are encoded separately, so change the view first\n",
    "        # (dim0, len(NODE_KEY_LIST), embedding_dim)\n",
    "        # -> (dim0, embedding_dim, len(NODE_KEY_LIST))\n",
    "        tmp0 = arg_seqs.view(tmp_dim0, len(NODE_KEY_LIST), self.embedding_dim).transpose(1,2)\n",
    "        # (dim0, n_kernels, len(NODE_KEY_LIST)-i)\n",
    "        tmp1s = [\n",
    "            F.relu(self.convs[i](tmp0))\n",
    "            for i in range(len(self.convs))\n",
    "        ]\n",
    "        # (dim0, n_kernels, 1)\n",
    "        tmp2s = [\n",
    "            self.pools[i](tmp1s[i])\n",
    "            for i in range(len(tmp1s))\n",
    "        ]\n",
    "        # (dim0, n_kernels)\n",
    "        tmp3s = [\n",
    "            tmp2s[i].view(tmp_dim0, self.n_kernels[i])\n",
    "            for i in range(len(tmp2s))\n",
    "        ]\n",
    "        tmp4 = torch.cat(tmp3s, 1) # (dim0, sum(n_kernels))\n",
    "        tmp5 = F.relu(self.linear(tmp4)) # (dim0, embedding_dim)\n",
    "        tmp6 = tmp5.view(B, tmpn_nodes, self.embedding_dim) # (B=1, ??, embedding_dim)\n",
    "        return tmp6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# note: ActionEncoder is actually encoding a list of actions\n",
    "#       not a single action\n",
    "class ActionEncoder(nn.Module):\n",
    "    def __init__(self, arg_embedding_dim):\n",
    "        super(ActionEncoder, self).__init__()\n",
    "        self.embedding_dim = arg_embedding_dim\n",
    "        self.n_kernels = [10, 10, 10]\n",
    "        self.kernel_sizes = [1, 2, 3]\n",
    "        self.convs = nn.ModuleList([\n",
    "            nn.Conv1d(\n",
    "                in_channels=arg_embedding_dim, \n",
    "                out_channels=self.n_kernels[i], \n",
    "                kernel_size=self.kernel_sizes[i], \n",
    "                padding=0\n",
    "            )\n",
    "            for i in range(len(self.kernel_sizes))\n",
    "        ])\n",
    "        self.pools = nn.ModuleList([\n",
    "            nn.MaxPool1d(\n",
    "                kernel_size=(\n",
    "                    len(NODE_KEY_LIST)+1-self.kernel_sizes[i],\n",
    "                ), \n",
    "                padding=0\n",
    "            )\n",
    "            for i in range(len(self.kernel_sizes))\n",
    "        ])\n",
    "        self.linear = nn.Linear(sum(self.n_kernels), arg_embedding_dim)\n",
    "        \n",
    "    # input a batch of sequences (high-dimensional)\n",
    "    # arg_seqs: (B=1, ??, len(NODE_KEY_LIST), embedding_dim)\n",
    "    # ??=others since it's encoding an action list\n",
    "    def forward(self, arg_seqs):\n",
    "        B = arg_seqs.shape[0]\n",
    "        tmpn_nodes = arg_seqs.shape[1]\n",
    "        tmp_dim0 = B * tmpn_nodes\n",
    "        assert B==1\n",
    "        # action nodes are encoded separately, so change the view first\n",
    "        # (dim0, len(NODE_KEY_LIST), embedding_dim)\n",
    "        # -> (dim0, embedding_dim, len(NODE_KEY_LIST))\n",
    "        tmp0 = arg_seqs.view(tmp_dim0, len(NODE_KEY_LIST), self.embedding_dim).transpose(1,2)\n",
    "        # (dim0, n_kernels, len(NODE_KEY_LIST)-i)\n",
    "        tmp1s = [\n",
    "            F.relu(self.convs[i](tmp0))\n",
    "            for i in range(len(self.convs))\n",
    "        ]\n",
    "        # (dim0, n_kernels, 1)\n",
    "        tmp2s = [\n",
    "            self.pools[i](tmp1s[i])\n",
    "            for i in range(len(tmp1s))\n",
    "        ]\n",
    "        # (dim0, n_kernels)\n",
    "        tmp3s = [\n",
    "            tmp2s[i].view(tmp_dim0, self.n_kernels[i])\n",
    "            for i in range(len(tmp2s))\n",
    "        ]\n",
    "        tmp4 = torch.cat(tmp3s, 1) # (dim0, sum(n_kernels))\n",
    "        # tmp5 = F.relu(self.linear(tmp4)) # (dim0, embedding_dim)\n",
    "        tmp5 = torch.sigmoid(self.linear(tmp4)) # (dim0, embedding_dim)\n",
    "        tmp6 = tmp5.view(B, tmpn_nodes, self.embedding_dim) # (B=1, ??, embedding_dim)\n",
    "        return tmp6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralAgent(nn.Module):\n",
    "    def __init__(self, arg_embedding_dim):\n",
    "        super(NeuralAgent, self).__init__()\n",
    "        self.embedding_dim = arg_embedding_dim\n",
    "        self.universal_token_encoder = UniversalTokenEncoder(arg_embedding_dim)\n",
    "        self.state_encoder = StateEncoder(arg_embedding_dim)\n",
    "        self.target_encoder = TargetEncoder(arg_embedding_dim)\n",
    "        self.action_encoder = ActionEncoder(arg_embedding_dim)\n",
    "        \n",
    "        self.hidden0 = nn.Linear(arg_embedding_dim*2, arg_embedding_dim)\n",
    "        \n",
    "    # arg_state: (B=1, ??=MAXN_STATE_NODES, len(NODE_KEY_LIST), MAX_TOKEN_LENGTH)\n",
    "    # arg_target: (B=1, ??=1, len(NODE_KEY_LIST), MAX_TOKEN_LENGTH)\n",
    "    def compute_preference(self, arg_state, arg_target):\n",
    "        B_state = arg_state.shape[0]\n",
    "        B_target = arg_target.shape[0]\n",
    "        assert B_state==1\n",
    "        assert B_target==1\n",
    "        \n",
    "        # fixme: only support 1 target at a time\n",
    "        n_targets = arg_target.shape[1]\n",
    "        assert n_targets==1 \n",
    "        \n",
    "        tmp0_state = self.universal_token_encoder(arg_state) # (B=1, ??, len(NODE_KEY_LIST), embedding_dim)\n",
    "        tmp0_target = self.universal_token_encoder(arg_target) # (B=1, ??, len(NODE_KEY_LIST), embedding_dim)\n",
    "        tmp1_state = self.state_encoder(tmp0_state) # (B=1, embedding_dim)\n",
    "        tmp1_target = self.target_encoder(tmp0_target) # (B=1, ??, embedding_dim)\n",
    "        tmp2_state = tmp1_state\n",
    "        \n",
    "        # fixme: only support 1 target at a time\n",
    "        tmp2_target = tmp1_target.view(B_target, self.embedding_dim) # (B=1, embedding_dim)\n",
    "        \n",
    "        tmp3 = torch.cat([tmp2_state, tmp2_target], 1) # (B=1, embedding_dim * 2)\n",
    "        tmp4 = torch.sigmoid(self.hidden0(tmp3)) # (B=1, embedding_dim)\n",
    "        return tmp4\n",
    "\n",
    "    # arg_action: (B=1, ??=others, len(NODE_KEY_LIST), MAX_TOKEN_LENGTH)\n",
    "    def encode_action_list(self, arg_action):\n",
    "        B = arg_action.shape[0]\n",
    "        tmp0 = self.universal_token_encoder(arg_action) # (B=1, ??, len(NODE_KEY_LIST), embedding_dim)\n",
    "        tmp1 = self.action_encoder(tmp0) # (B=1, ??, embedding_dim)\n",
    "        return tmp1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_reward0(arg_env, arg_state, arg_target):\n",
    "    return 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. roll out an action sequence\n",
    "# 2. compute reward\n",
    "# 3. policy gradient back propagation\n",
    "def rollout(arg_env, arg_agent, arg_optimizer, arg_maxn_steps, arg_target):\n",
    "    # note: remember to clear the state\n",
    "    arg_env.launch_app()\n",
    "    \n",
    "    rollout_outputs = []\n",
    "    rollout_actions = []\n",
    "    rollout_action_ids = []\n",
    "    \n",
    "    for i in range(arg_maxn_steps):\n",
    "        time.sleep(1)\n",
    "        state_i = arg_env.get_current_state()\n",
    "        \n",
    "        if get_reward0(arg_env, state_i, arg_target) == 1:\n",
    "            print(\"# reach target state after {} actions\".format(len(rollout_outputs)))\n",
    "            # already reach the target state\n",
    "            break\n",
    "            \n",
    "        action_list = arg_env.get_available_actionable_elements(state_i)\n",
    "        n_actions = len(action_list)\n",
    "        if n_actions == 0:\n",
    "            print(\"# no action is found, terminate.\")\n",
    "            # no available actions any more\n",
    "            break\n",
    "          \n",
    "        # should wrap [] to make B=1\n",
    "        # (B=1, ??=MAXN_STATE_NODES, len(NODE_KEY_LIST), MAX_TOKEN_LENGTH)\n",
    "        state_matrix_i = np.asarray([get_state_matrix(state_i)])\n",
    "        # (B=1, ??=1, len(NODE_KEY_LIST), MAX_TOKEN_LENGTH)\n",
    "        target_matrix_i = np.asarray([[\n",
    "            get_sentence_vector(arg_target)\n",
    "        ]])\n",
    "        # (B=1, ??=others, len(NODE_KEY_LIST), MAX_TOKEN_LENGTH)\n",
    "        action_matrix_i = np.asarray([[\n",
    "            get_element_vector(action_list[j])\n",
    "            for j in range(len(action_list))\n",
    "        ]])\n",
    "        \n",
    "        print(\"state: {}\".format(state_matrix_i.shape))\n",
    "        print(\"target: {}\".format(target_matrix_i.shape))\n",
    "        print(\"action: {}\".format(action_matrix_i.shape))\n",
    "        \n",
    "        t_state = Variable(torch.tensor(state_matrix_i, dtype=torch.long).to(device))\n",
    "        t_target = Variable(torch.tensor(target_matrix_i, dtype=torch.long).to(device))\n",
    "        t_action = Variable(torch.tensor(action_matrix_i, dtype=torch.long).to(device))\n",
    "        \n",
    "        B_state = t_state.shape[0]\n",
    "        B_target = t_target.shape[0]\n",
    "        B_action = t_action.shape[0]\n",
    "        assert B_state==1\n",
    "        assert B_target==1\n",
    "        assert B_action==1\n",
    "        \n",
    "        arg_agent.train()\n",
    "        tout_preference = arg_agent.compute_preference(t_state, t_target) # (B=1, embedding_dim)\n",
    "        tout_action = arg_agent.encode_action_list(t_action) # (B=1, ??, embedding_dim)\n",
    "        \n",
    "        print(\"tout_preference.shape={}\".format(tout_preference.shape))\n",
    "        print(\"tout_action.shape={}\".format(tout_action.shape))\n",
    "        \n",
    "        # ====> using cosine similarity\n",
    "        # (n_actions, spec_dims)\n",
    "        # t0_output = t_output.expand_as(t_pool)\n",
    "        # t_cos = F.cosine_similarity(t0_output, t_pool, dim=1)\n",
    "        # t_act = F.log_softmax(t_cos, dim=0)\n",
    "        # ====> directly mm similarity\n",
    "        # note: assuming B=1 already\n",
    "        tout0_preference = tout_preference.view(-1,1) # (embedding_dim, 1)\n",
    "        tout0_action = tout_action.view(-1, EMBEDDING_DIM) # (n_actions, embedding_dim)\n",
    "        tout0_mm = torch.mm(tout0_action, tout0_preference)  # (n_actions, 1)\n",
    "        tout1_mm = tout0_mm.view(-1) # (n_actions,)\n",
    "        tout2_mm = F.log_softmax(tout1_mm)\n",
    "        print(\"# tout2_mm: {}\".format(tout2_mm))\n",
    "        \n",
    "        if random.random()>0.8:\n",
    "            # explore\n",
    "            selected_action_id = random.choice(list(range(len(action_list))))\n",
    "            print(\"# selected_action_id (rnd): {}, log-sim: {}\".format(selected_action_id, tout2_mm[selected_action_id]))\n",
    "        else:\n",
    "            # exploit\n",
    "            probs = tout2_mm.exp().tolist()\n",
    "            selected_action_id = random.choices(list(range(len(action_list))), weights=probs, k=1)[0]\n",
    "            # selected_action_id = torch.argmax(tout2_mm, dim=0).tolist()\n",
    "            print(\"# selected_action_id (mul): {}, log-sim: {}\".format(selected_action_id, tout2_mm[selected_action_id]))\n",
    "        \n",
    "        # perform action\n",
    "        arg_env.perform_action(action_list[selected_action_id])\n",
    "        next_state = arg_env.get_current_state()\n",
    "        \n",
    "        # store the choices\n",
    "        rollout_outputs.append(tout2_mm)\n",
    "        rollout_actions.append(action_list)\n",
    "        rollout_action_ids.append(selected_action_id)\n",
    "        \n",
    "        # input(\"PAUSE\")\n",
    "        \n",
    "    # here we use the final reward as the cumulative reward\n",
    "    final_state = arg_env.get_current_state()\n",
    "    final_reward = get_reward0(arg_env, final_state, arg_target)\n",
    "    print(\"# final reward: {}\".format(final_reward))\n",
    "    \n",
    "    rollout_loss = []\n",
    "    current_reward = final_reward\n",
    "    # reverse from the last to first\n",
    "    for i in range(len(rollout_outputs))[::-1]:\n",
    "        rollout_loss.append( current_reward * (-rollout_outputs[i][rollout_action_ids[i]]) )\n",
    "        current_reward *= 0.8 # decay\n",
    "    rollout_loss = rollout_loss[::-1]\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss = sum(rollout_loss)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. roll out an action sequence\n",
    "# 2. compute reward\n",
    "# 3. policy gradient back propagation\n",
    "def random_rollout(arg_env, arg_maxn_steps, arg_target):\n",
    "    # note: remember to clear the state\n",
    "    arg_env.launch_app()\n",
    "    \n",
    "    for i in range(arg_maxn_steps):\n",
    "        # time.sleep(1)\n",
    "        state_i = arg_env.get_current_state()\n",
    "        \n",
    "        if get_reward0(arg_env, state_i, arg_target) == 1:\n",
    "            print(\"# reach target state after {} actions\".format(len(rollout_outputs)))\n",
    "            # already reach the target state\n",
    "            break\n",
    "            \n",
    "        action_list = arg_env.get_available_actionable_elements(state_i)\n",
    "        n_actions = len(action_list)\n",
    "        if n_actions == 0:\n",
    "            print(\"# no action is found, terminate.\")\n",
    "            # no available actions any more\n",
    "            break\n",
    "            \n",
    "        selected_action_id = random.choice(list(range(len(action_list))))\n",
    "          \n",
    "        # perform action\n",
    "        arg_env.perform_action(action_list[selected_action_id])\n",
    "        next_state = arg_env.get_current_state()\n",
    "        \n",
    "        # input(\"PAUSE\")\n",
    "        \n",
    "    # here we use the final reward as the cumulative reward\n",
    "    final_state = arg_env.get_current_state()\n",
    "    final_reward = get_reward0(arg_env, final_state, arg_target)\n",
    "    print(\"# final reward: {}\".format(final_reward))\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Top-Level Control Flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m[#] Analysis started at: 2021-02-25 02:58:14 AM\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] | 2021-02-25 02:58:14 AM | analyzer.testapp_2 | Adb is running with root priviledges now!\n",
      "[WARNING] | 2021-02-25 02:58:18 AM | analyzer.testapp_2 | Error in logcat cleaning\n",
      "[INFO] | 2021-02-25 02:58:19 AM | analyzer.testapp_2 | APK installtion done for testapp_2.apk\n",
      "[INFO] | 2021-02-25 02:58:19 AM | analyzer.testapp_2 | Kill the current app if already spawned!\n",
      "[INFO] | 2021-02-25 02:58:20 AM | analyzer.testapp_2 | APK is already killed\n",
      "[INFO] | 2021-02-25 02:58:20 AM | analyzer.testapp_2 | Spawning the current app\n",
      "[INFO] | 2021-02-25 02:58:20 AM | analyzer.testapp_2 | Apk spawned successfully!\n",
      "[INFO] | 2021-02-25 02:58:23 AM | analyzer.testapp_2 | Old logcat messages cleared!\n"
     ]
    }
   ],
   "source": [
    "from main import *\n",
    "\n",
    "CURR_DIR = os.path.dirname(os.getcwd())\n",
    "OUTPUT_DIR = os.path.join(CURR_DIR, \"results\")\n",
    "\n",
    "args = {\n",
    "#     \"path\": \"../results/test_app_1/testapp_1.apk\",\n",
    "    \"path\": \"../results/test_app_2/testapp_2.apk\",\n",
    "#     \"path\": \"../test/com.github.cetoolbox_11/app_simple0.apk\",\n",
    "#     \"path\": \"/Users/joseph/Desktop/UCSB/20summer/MarthaEnv/tmp/Wordpress_394/Wordpress_394.apk\",\n",
    "#     \"path\": \"/Users/joseph/Desktop/UCSB/20summer/MarthaEnv/tmp/com.zoffcc.applications.aagtl_31/com.zoffcc.applications.aagtl_31.apk\",\n",
    "#     \"path\": \"/Users/joseph/Desktop/UCSB/20summer/MarthaEnv/tmp/Translate/Translate.apk\",\n",
    "#     \"path\": \"/Users/joseph/Desktop/UCSB/20summer/MarthaEnv/tmp/com.chmod0.manpages_3/com.chmod0.manpages_3.apk\",\n",
    "#     \"path\": \"/Users/joseph/Desktop/UCSB/20summer/MarthaEnv/tmp/Book-Catalogue/Book-Catalogue.apk\",\n",
    "#     \"path\": \"/Users/joseph/Desktop/UCSB/20summer/MarthaEnv/test/out.andFHEM.apk\",\n",
    "#     \"path\": \"/Users/joseph/Desktop/UCSB/20summer/MarthaEnv/test/out.blue-chat.apk\",\n",
    "#     \"path\": \"/Users/joseph/Desktop/UCSB/20summer/MarthaEnv/test/out.CallMeter3G-debug.apk\",\n",
    "#     \"path\": \"/Users/joseph/Desktop/UCSB/20summer/MarthaEnv/test/out.Lucid-Browser.apk\",\n",
    "    \"output\": \"../results/\",\n",
    "}\n",
    "\n",
    "if args[\"path\"] is not None:\n",
    "    pyaxmlparser_apk = APK(args[\"path\"])\n",
    "    apk_base_name = os.path.splitext(os.path.basename(args[\"path\"]))[0]\n",
    "\n",
    "else:\n",
    "    parser.print_usage()\n",
    "\n",
    "if args[\"output\"] is not None:\n",
    "    OUTPUT_DIR = args[\"output\"]\n",
    "\n",
    "output_dir = os.path.join(OUTPUT_DIR, apk_base_name)\n",
    "\n",
    "if os.path.exists(output_dir):\n",
    "    rmtree(output_dir)\n",
    "\n",
    "if not os.path.exists(output_dir):\n",
    "    os.mkdir(output_dir)\n",
    "\n",
    "# Setting the path for log file\n",
    "log_path = os.path.join(output_dir, 'analysis.log')\n",
    "log = init_logging('analyzer.%s' % apk_base_name, log_path, file_mode='w', console=True)\n",
    "\n",
    "# Record analysis start time\n",
    "now = datetime.datetime.now()\n",
    "analysis_start_time = now.strftime(DATE_FORMAT)\n",
    "info('Analysis started at: %s' % analysis_start_time)\n",
    "start_time = time.time()\n",
    "\n",
    "# Get the serial for the device attached to ADB\n",
    "device_serial = get_device_serial(log)\n",
    "\n",
    "if device_serial is None:\n",
    "    log.warning(\"Device is not connected!\")\n",
    "    sys.exit(1)\n",
    "\n",
    "# Initialize the uiautomator device object using the device serial\n",
    "uiautomator_device = u2.connect(device_serial)\n",
    "run_adb_as_root(log)\n",
    "apk = Apk(args[\"path\"], uiautomator_device, output_dir, log)\n",
    "apk.launch_app()\n",
    "# to track some goal state at startup, you don't have to do this\n",
    "apk.clean_logcat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] | 2021-02-25 02:03:04 AM | analyzer.app_simple0 | Kill the current app if already spawned!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# ep0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] | 2021-02-25 02:03:04 AM | analyzer.app_simple0 | APK killed\n",
      "[INFO] | 2021-02-25 02:03:04 AM | analyzer.app_simple0 | Spawning the current app\n",
      "[INFO] | 2021-02-25 02:03:05 AM | analyzer.app_simple0 | Apk spawned successfully!\n",
      "[D 210225 02:03:08 __init__:600] kill process(ps): uiautomator\n",
      "[D 210225 02:03:10 __init__:619] uiautomator-v2 is starting ... left: 40.0s\n",
      "[D 210225 02:03:11 __init__:619] uiautomator-v2 is starting ... left: 38.7s\n",
      "[D 210225 02:03:12 __init__:619] uiautomator-v2 is starting ... left: 37.5s\n",
      "[D 210225 02:03:14 __init__:619] uiautomator-v2 is starting ... left: 36.3s\n",
      "[D 210225 02:03:15 __init__:619] uiautomator-v2 is starting ... left: 35.0s\n",
      "[D 210225 02:03:16 __init__:619] uiautomator-v2 is starting ... left: 33.8s\n",
      "[I 210225 02:03:16 __init__:583] uiautomator back to normal\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state: (1, 400, 17, 20)\n",
      "target: (1, 1, 17, 20)\n",
      "action: (1, 14, 17, 20)\n",
      "tout_preference.shape=torch.Size([1, 16])\n",
      "tout_action.shape=torch.Size([1, 14, 16])\n",
      "# tout2_mm: tensor([-2.6470, -2.6342, -2.6242, -2.6300, -2.6365, -2.6383, -2.6331, -2.6454,\n",
      "        -2.6336, -2.6282, -2.6342, -2.6529, -2.6520, -2.6578],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "# selected_action_id (mul): 5, log-sim: -2.63834810256958\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/joseph/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:74: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state: (1, 400, 17, 20)\n",
      "target: (1, 1, 17, 20)\n",
      "action: (1, 14, 17, 20)\n",
      "tout_preference.shape=torch.Size([1, 16])\n",
      "tout_action.shape=torch.Size([1, 14, 16])\n",
      "# tout2_mm: tensor([-2.6470, -2.6342, -2.6242, -2.6300, -2.6365, -2.6383, -2.6331, -2.6454,\n",
      "        -2.6336, -2.6282, -2.6342, -2.6529, -2.6520, -2.6578],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "# selected_action_id (mul): 12, log-sim: -2.6519765853881836\n",
      "state: (1, 400, 17, 20)\n",
      "target: (1, 1, 17, 20)\n",
      "action: (1, 10, 17, 20)\n",
      "tout_preference.shape=torch.Size([1, 16])\n",
      "tout_action.shape=torch.Size([1, 10, 16])\n",
      "# tout2_mm: tensor([-2.3055, -2.2955, -2.2991, -2.3048, -2.2995, -2.2942, -2.2977, -2.3086,\n",
      "        -2.3076, -2.3134], grad_fn=<LogSoftmaxBackward>)\n",
      "# selected_action_id (mul): 3, log-sim: -2.3048315048217773\n",
      "state: (1, 400, 17, 20)\n",
      "target: (1, 1, 17, 20)\n",
      "action: (1, 10, 17, 20)\n",
      "tout_preference.shape=torch.Size([1, 16])\n",
      "tout_action.shape=torch.Size([1, 10, 16])\n",
      "# tout2_mm: tensor([-2.3055, -2.2955, -2.2991, -2.3048, -2.2995, -2.2942, -2.2977, -2.3086,\n",
      "        -2.3076, -2.3134], grad_fn=<LogSoftmaxBackward>)\n",
      "# selected_action_id (mul): 6, log-sim: -2.297739028930664\n",
      "state: (1, 400, 17, 20)\n",
      "target: (1, 1, 17, 20)\n",
      "action: (1, 10, 17, 20)\n",
      "tout_preference.shape=torch.Size([1, 16])\n",
      "tout_action.shape=torch.Size([1, 10, 16])\n",
      "# tout2_mm: tensor([-2.3055, -2.2955, -2.2991, -2.3048, -2.2995, -2.2942, -2.2977, -2.3086,\n",
      "        -2.3076, -2.3134], grad_fn=<LogSoftmaxBackward>)\n",
      "# selected_action_id (rnd): 5, log-sim: -2.294154167175293\n",
      "# final reward: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] | 2021-02-25 02:03:36 AM | analyzer.app_simple0 | Kill the current app if already spawned!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# ep1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] | 2021-02-25 02:03:36 AM | analyzer.app_simple0 | APK killed\n",
      "[INFO] | 2021-02-25 02:03:36 AM | analyzer.app_simple0 | Spawning the current app\n",
      "[INFO] | 2021-02-25 02:03:37 AM | analyzer.app_simple0 | Apk spawned successfully!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state: (1, 400, 17, 20)\n",
      "target: (1, 1, 17, 20)\n",
      "action: (1, 14, 17, 20)\n",
      "tout_preference.shape=torch.Size([1, 16])\n",
      "tout_action.shape=torch.Size([1, 14, 16])\n",
      "# tout2_mm: tensor([-2.6470, -2.6342, -2.6242, -2.6300, -2.6365, -2.6383, -2.6331, -2.6454,\n",
      "        -2.6336, -2.6282, -2.6342, -2.6529, -2.6520, -2.6578],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "# selected_action_id (mul): 3, log-sim: -2.630032777786255\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/joseph/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:74: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state: (1, 400, 17, 20)\n",
      "target: (1, 1, 17, 20)\n",
      "action: (1, 14, 17, 20)\n",
      "tout_preference.shape=torch.Size([1, 16])\n",
      "tout_action.shape=torch.Size([1, 14, 16])\n",
      "# tout2_mm: tensor([-2.6470, -2.6342, -2.6242, -2.6300, -2.6365, -2.6383, -2.6331, -2.6454,\n",
      "        -2.6336, -2.6282, -2.6342, -2.6529, -2.6520, -2.6578],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "# selected_action_id (mul): 12, log-sim: -2.6519765853881836\n",
      "state: (1, 400, 17, 20)\n",
      "target: (1, 1, 17, 20)\n",
      "action: (1, 10, 17, 20)\n",
      "tout_preference.shape=torch.Size([1, 16])\n",
      "tout_action.shape=torch.Size([1, 10, 16])\n",
      "# tout2_mm: tensor([-2.3055, -2.2955, -2.2991, -2.3048, -2.2995, -2.2942, -2.2977, -2.3086,\n",
      "        -2.3076, -2.3134], grad_fn=<LogSoftmaxBackward>)\n",
      "# selected_action_id (mul): 5, log-sim: -2.294153928756714\n",
      "state: (1, 400, 17, 20)\n",
      "target: (1, 1, 17, 20)\n",
      "action: (1, 10, 17, 20)\n",
      "tout_preference.shape=torch.Size([1, 16])\n",
      "tout_action.shape=torch.Size([1, 10, 16])\n",
      "# tout2_mm: tensor([-2.3055, -2.2955, -2.2991, -2.3048, -2.2995, -2.2942, -2.2977, -2.3086,\n",
      "        -2.3076, -2.3134], grad_fn=<LogSoftmaxBackward>)\n",
      "# selected_action_id (mul): 5, log-sim: -2.294154167175293\n",
      "state: (1, 400, 17, 20)\n",
      "target: (1, 1, 17, 20)\n",
      "action: (1, 10, 17, 20)\n",
      "tout_preference.shape=torch.Size([1, 16])\n",
      "tout_action.shape=torch.Size([1, 10, 16])\n",
      "# tout2_mm: tensor([-2.3055, -2.2955, -2.2991, -2.3048, -2.2995, -2.2942, -2.2977, -2.3086,\n",
      "        -2.3076, -2.3134], grad_fn=<LogSoftmaxBackward>)\n",
      "# selected_action_id (mul): 7, log-sim: -2.3086166381835938\n",
      "# final reward: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] | 2021-02-25 02:03:58 AM | analyzer.app_simple0 | Kill the current app if already spawned!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# ep2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] | 2021-02-25 02:03:59 AM | analyzer.app_simple0 | APK killed\n",
      "[INFO] | 2021-02-25 02:03:59 AM | analyzer.app_simple0 | Spawning the current app\n",
      "[INFO] | 2021-02-25 02:03:59 AM | analyzer.app_simple0 | Apk spawned successfully!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state: (1, 400, 17, 20)\n",
      "target: (1, 1, 17, 20)\n",
      "action: (1, 14, 17, 20)\n",
      "tout_preference.shape=torch.Size([1, 16])\n",
      "tout_action.shape=torch.Size([1, 14, 16])\n",
      "# tout2_mm: tensor([-2.6470, -2.6342, -2.6242, -2.6300, -2.6365, -2.6383, -2.6331, -2.6454,\n",
      "        -2.6336, -2.6282, -2.6342, -2.6530, -2.6520, -2.6578],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "# selected_action_id (mul): 4, log-sim: -2.6365323066711426\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/joseph/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:74: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state: (1, 400, 17, 20)\n",
      "target: (1, 1, 17, 20)\n",
      "action: (1, 14, 17, 20)\n",
      "tout_preference.shape=torch.Size([1, 16])\n",
      "tout_action.shape=torch.Size([1, 14, 16])\n",
      "# tout2_mm: tensor([-2.6470, -2.6342, -2.6242, -2.6300, -2.6365, -2.6383, -2.6331, -2.6454,\n",
      "        -2.6336, -2.6282, -2.6342, -2.6530, -2.6520, -2.6578],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "# selected_action_id (mul): 0, log-sim: -2.646989583969116\n",
      "state: (1, 400, 17, 20)\n",
      "target: (1, 1, 17, 20)\n",
      "action: (1, 14, 17, 20)\n",
      "tout_preference.shape=torch.Size([1, 16])\n",
      "tout_action.shape=torch.Size([1, 14, 16])\n",
      "# tout2_mm: tensor([-2.6440, -2.6286, -2.6249, -2.6362, -2.6342, -2.6377, -2.6355, -2.6419,\n",
      "        -2.6353, -2.6314, -2.6383, -2.6518, -2.6509, -2.6567],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "# selected_action_id (mul): 4, log-sim: -2.634215831756592\n",
      "state: (1, 400, 17, 20)\n",
      "target: (1, 1, 17, 20)\n",
      "action: (1, 14, 17, 20)\n",
      "tout_preference.shape=torch.Size([1, 16])\n",
      "tout_action.shape=torch.Size([1, 14, 16])\n",
      "# tout2_mm: tensor([-2.6440, -2.6286, -2.6249, -2.6362, -2.6342, -2.6377, -2.6355, -2.6419,\n",
      "        -2.6353, -2.6314, -2.6383, -2.6518, -2.6509, -2.6567],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "# selected_action_id (rnd): 8, log-sim: -2.635275363922119\n",
      "state: (1, 400, 17, 20)\n",
      "target: (1, 1, 17, 20)\n",
      "action: (1, 14, 17, 20)\n",
      "tout_preference.shape=torch.Size([1, 16])\n",
      "tout_action.shape=torch.Size([1, 14, 16])\n",
      "# tout2_mm: tensor([-2.6440, -2.6286, -2.6249, -2.6362, -2.6342, -2.6377, -2.6355, -2.6419,\n",
      "        -2.6353, -2.6314, -2.6383, -2.6518, -2.6509, -2.6567],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "# selected_action_id (mul): 7, log-sim: -2.6419126987457275\n",
      "# final reward: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] | 2021-02-25 02:04:21 AM | analyzer.app_simple0 | Kill the current app if already spawned!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# ep3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] | 2021-02-25 02:04:21 AM | analyzer.app_simple0 | APK killed\n",
      "[INFO] | 2021-02-25 02:04:21 AM | analyzer.app_simple0 | Spawning the current app\n",
      "[INFO] | 2021-02-25 02:04:22 AM | analyzer.app_simple0 | Apk spawned successfully!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state: (1, 400, 17, 20)\n",
      "target: (1, 1, 17, 20)\n",
      "action: (1, 14, 17, 20)\n",
      "tout_preference.shape=torch.Size([1, 16])\n",
      "tout_action.shape=torch.Size([1, 14, 16])\n",
      "# tout2_mm: tensor([-2.6470, -2.6342, -2.6242, -2.6300, -2.6365, -2.6383, -2.6331, -2.6454,\n",
      "        -2.6336, -2.6282, -2.6342, -2.6530, -2.6520, -2.6578],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "# selected_action_id (mul): 6, log-sim: -2.6330814361572266\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/joseph/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:74: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state: (1, 400, 17, 20)\n",
      "target: (1, 1, 17, 20)\n",
      "action: (1, 2, 17, 20)\n",
      "tout_preference.shape=torch.Size([1, 16])\n",
      "tout_action.shape=torch.Size([1, 2, 16])\n",
      "# tout2_mm: tensor([-0.6864, -0.7000], grad_fn=<LogSoftmaxBackward>)\n",
      "# selected_action_id (mul): 1, log-sim: -0.6999553442001343\n",
      "state: (1, 400, 17, 20)\n",
      "target: (1, 1, 17, 20)\n",
      "action: (1, 14, 17, 20)\n",
      "tout_preference.shape=torch.Size([1, 16])\n",
      "tout_action.shape=torch.Size([1, 14, 16])\n",
      "# tout2_mm: tensor([-2.6470, -2.6342, -2.6242, -2.6300, -2.6365, -2.6383, -2.6331, -2.6454,\n",
      "        -2.6336, -2.6282, -2.6342, -2.6530, -2.6520, -2.6578],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "# selected_action_id (mul): 5, log-sim: -2.638343095779419\n",
      "state: (1, 400, 17, 20)\n",
      "target: (1, 1, 17, 20)\n",
      "action: (1, 14, 17, 20)\n",
      "tout_preference.shape=torch.Size([1, 16])\n",
      "tout_action.shape=torch.Size([1, 14, 16])\n",
      "# tout2_mm: tensor([-2.6470, -2.6342, -2.6242, -2.6300, -2.6365, -2.6383, -2.6331, -2.6454,\n",
      "        -2.6336, -2.6282, -2.6342, -2.6530, -2.6520, -2.6578],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "# selected_action_id (mul): 5, log-sim: -2.638343095779419\n",
      "state: (1, 400, 17, 20)\n",
      "target: (1, 1, 17, 20)\n",
      "action: (1, 14, 17, 20)\n",
      "tout_preference.shape=torch.Size([1, 16])\n",
      "tout_action.shape=torch.Size([1, 14, 16])\n",
      "# tout2_mm: tensor([-2.6470, -2.6342, -2.6242, -2.6300, -2.6365, -2.6383, -2.6331, -2.6454,\n",
      "        -2.6336, -2.6282, -2.6342, -2.6530, -2.6520, -2.6578],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "# selected_action_id (rnd): 6, log-sim: -2.6330814361572266\n",
      "# final reward: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] | 2021-02-25 02:04:42 AM | analyzer.app_simple0 | Kill the current app if already spawned!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# ep4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] | 2021-02-25 02:04:42 AM | analyzer.app_simple0 | APK killed\n",
      "[INFO] | 2021-02-25 02:04:42 AM | analyzer.app_simple0 | Spawning the current app\n",
      "[INFO] | 2021-02-25 02:04:43 AM | analyzer.app_simple0 | Apk spawned successfully!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state: (1, 400, 17, 20)\n",
      "target: (1, 1, 17, 20)\n",
      "action: (1, 14, 17, 20)\n",
      "tout_preference.shape=torch.Size([1, 16])\n",
      "tout_action.shape=torch.Size([1, 14, 16])\n",
      "# tout2_mm: tensor([-2.6470, -2.6342, -2.6242, -2.6300, -2.6365, -2.6383, -2.6331, -2.6454,\n",
      "        -2.6336, -2.6282, -2.6342, -2.6530, -2.6520, -2.6578],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "# selected_action_id (mul): 5, log-sim: -2.638343095779419\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/joseph/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:74: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state: (1, 400, 17, 20)\n",
      "target: (1, 1, 17, 20)\n",
      "action: (1, 14, 17, 20)\n",
      "tout_preference.shape=torch.Size([1, 16])\n",
      "tout_action.shape=torch.Size([1, 14, 16])\n",
      "# tout2_mm: tensor([-2.6470, -2.6342, -2.6242, -2.6300, -2.6365, -2.6383, -2.6331, -2.6454,\n",
      "        -2.6336, -2.6282, -2.6342, -2.6530, -2.6520, -2.6578],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "# selected_action_id (mul): 2, log-sim: -2.6242308616638184\n",
      "state: (1, 400, 17, 20)\n",
      "target: (1, 1, 17, 20)\n",
      "action: (1, 2, 17, 20)\n",
      "tout_preference.shape=torch.Size([1, 16])\n",
      "tout_action.shape=torch.Size([1, 2, 16])\n",
      "# tout2_mm: tensor([-0.6992, -0.6871], grad_fn=<LogSoftmaxBackward>)\n",
      "# selected_action_id (mul): 0, log-sim: -0.699240505695343\n",
      "state: (1, 400, 17, 20)\n",
      "target: (1, 1, 17, 20)\n",
      "action: (1, 14, 17, 20)\n",
      "tout_preference.shape=torch.Size([1, 16])\n",
      "tout_action.shape=torch.Size([1, 14, 16])\n",
      "# tout2_mm: tensor([-2.6470, -2.6342, -2.6242, -2.6300, -2.6365, -2.6383, -2.6331, -2.6454,\n",
      "        -2.6336, -2.6282, -2.6342, -2.6530, -2.6520, -2.6578],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "# selected_action_id (mul): 2, log-sim: -2.6242308616638184\n",
      "state: (1, 400, 17, 20)\n",
      "target: (1, 1, 17, 20)\n",
      "action: (1, 2, 17, 20)\n",
      "tout_preference.shape=torch.Size([1, 16])\n",
      "tout_action.shape=torch.Size([1, 2, 16])\n",
      "# tout2_mm: tensor([-0.6992, -0.6871], grad_fn=<LogSoftmaxBackward>)\n",
      "# selected_action_id (mul): 1, log-sim: -0.6870927214622498\n",
      "# final reward: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] | 2021-02-25 02:05:04 AM | analyzer.app_simple0 | Kill the current app if already spawned!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# ep5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] | 2021-02-25 02:05:04 AM | analyzer.app_simple0 | APK killed\n",
      "[INFO] | 2021-02-25 02:05:04 AM | analyzer.app_simple0 | Spawning the current app\n",
      "[INFO] | 2021-02-25 02:05:04 AM | analyzer.app_simple0 | Apk spawned successfully!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state: (1, 400, 17, 20)\n",
      "target: (1, 1, 17, 20)\n",
      "action: (1, 14, 17, 20)\n",
      "tout_preference.shape=torch.Size([1, 16])\n",
      "tout_action.shape=torch.Size([1, 14, 16])\n",
      "# tout2_mm: tensor([-2.6470, -2.6342, -2.6242, -2.6300, -2.6365, -2.6383, -2.6331, -2.6454,\n",
      "        -2.6336, -2.6282, -2.6342, -2.6530, -2.6520, -2.6578],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "# selected_action_id (mul): 6, log-sim: -2.633082866668701\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/joseph/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:74: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state: (1, 400, 17, 20)\n",
      "target: (1, 1, 17, 20)\n",
      "action: (1, 2, 17, 20)\n",
      "tout_preference.shape=torch.Size([1, 16])\n",
      "tout_action.shape=torch.Size([1, 2, 16])\n",
      "# tout2_mm: tensor([-0.6864, -0.7000], grad_fn=<LogSoftmaxBackward>)\n",
      "# selected_action_id (rnd): 0, log-sim: -0.6863829493522644\n",
      "state: (1, 400, 17, 20)\n",
      "target: (1, 1, 17, 20)\n",
      "action: (1, 14, 17, 20)\n",
      "tout_preference.shape=torch.Size([1, 16])\n",
      "tout_action.shape=torch.Size([1, 14, 16])\n",
      "# tout2_mm: tensor([-2.6470, -2.6342, -2.6242, -2.6300, -2.6365, -2.6383, -2.6331, -2.6454,\n",
      "        -2.6336, -2.6282, -2.6342, -2.6530, -2.6520, -2.6578],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "# selected_action_id (mul): 9, log-sim: -2.6282386779785156\n",
      "state: (1, 400, 17, 20)\n",
      "target: (1, 1, 17, 20)\n",
      "action: (1, 14, 17, 20)\n",
      "tout_preference.shape=torch.Size([1, 16])\n",
      "tout_action.shape=torch.Size([1, 14, 16])\n",
      "# tout2_mm: tensor([-2.6470, -2.6342, -2.6242, -2.6300, -2.6365, -2.6383, -2.6331, -2.6454,\n",
      "        -2.6336, -2.6282, -2.6342, -2.6530, -2.6520, -2.6578],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "# selected_action_id (mul): 2, log-sim: -2.6242308616638184\n",
      "state: (1, 400, 17, 20)\n",
      "target: (1, 1, 17, 20)\n",
      "action: (1, 2, 17, 20)\n",
      "tout_preference.shape=torch.Size([1, 16])\n",
      "tout_action.shape=torch.Size([1, 2, 16])\n",
      "# tout2_mm: tensor([-0.6992, -0.6871], grad_fn=<LogSoftmaxBackward>)\n",
      "# selected_action_id (mul): 1, log-sim: -0.6870936751365662\n",
      "# final reward: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] | 2021-02-25 02:05:25 AM | analyzer.app_simple0 | Kill the current app if already spawned!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# ep6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] | 2021-02-25 02:05:26 AM | analyzer.app_simple0 | APK killed\n",
      "[INFO] | 2021-02-25 02:05:26 AM | analyzer.app_simple0 | Spawning the current app\n",
      "[INFO] | 2021-02-25 02:05:26 AM | analyzer.app_simple0 | Apk spawned successfully!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state: (1, 400, 17, 20)\n",
      "target: (1, 1, 17, 20)\n",
      "action: (1, 14, 17, 20)\n",
      "tout_preference.shape=torch.Size([1, 16])\n",
      "tout_action.shape=torch.Size([1, 14, 16])\n",
      "# tout2_mm: tensor([-2.6470, -2.6342, -2.6242, -2.6300, -2.6365, -2.6383, -2.6331, -2.6454,\n",
      "        -2.6336, -2.6282, -2.6342, -2.6530, -2.6520, -2.6578],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "# selected_action_id (mul): 0, log-sim: -2.6469926834106445\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/joseph/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:74: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state: (1, 400, 17, 20)\n",
      "target: (1, 1, 17, 20)\n",
      "action: (1, 14, 17, 20)\n",
      "tout_preference.shape=torch.Size([1, 16])\n",
      "tout_action.shape=torch.Size([1, 14, 16])\n",
      "# tout2_mm: tensor([-2.6440, -2.6286, -2.6249, -2.6362, -2.6342, -2.6377, -2.6355, -2.6419,\n",
      "        -2.6353, -2.6314, -2.6383, -2.6518, -2.6509, -2.6567],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "# selected_action_id (rnd): 12, log-sim: -2.6508591175079346\n",
      "state: (1, 400, 17, 20)\n",
      "target: (1, 1, 17, 20)\n",
      "action: (1, 10, 17, 20)\n",
      "tout_preference.shape=torch.Size([1, 16])\n",
      "tout_action.shape=torch.Size([1, 10, 16])\n",
      "# tout2_mm: tensor([-2.3055, -2.2955, -2.2991, -2.3048, -2.2995, -2.2942, -2.2977, -2.3086,\n",
      "        -2.3076, -2.3134], grad_fn=<LogSoftmaxBackward>)\n",
      "# selected_action_id (rnd): 3, log-sim: -2.3048317432403564\n",
      "state: (1, 400, 17, 20)\n",
      "target: (1, 1, 17, 20)\n",
      "action: (1, 10, 17, 20)\n",
      "tout_preference.shape=torch.Size([1, 16])\n",
      "tout_action.shape=torch.Size([1, 10, 16])\n",
      "# tout2_mm: tensor([-2.3055, -2.2955, -2.2991, -2.3048, -2.2995, -2.2942, -2.2977, -2.3086,\n",
      "        -2.3076, -2.3134], grad_fn=<LogSoftmaxBackward>)\n",
      "# selected_action_id (mul): 8, log-sim: -2.307647466659546\n",
      "state: (1, 400, 17, 20)\n",
      "target: (1, 1, 17, 20)\n",
      "action: (1, 10, 17, 20)\n",
      "tout_preference.shape=torch.Size([1, 16])\n",
      "tout_action.shape=torch.Size([1, 10, 16])\n",
      "# tout2_mm: tensor([-2.3055, -2.2955, -2.2991, -2.3048, -2.2995, -2.2942, -2.2977, -2.3086,\n",
      "        -2.3076, -2.3134], grad_fn=<LogSoftmaxBackward>)\n",
      "# selected_action_id (mul): 0, log-sim: -2.305535316467285\n",
      "# final reward: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] | 2021-02-25 02:05:51 AM | analyzer.app_simple0 | Kill the current app if already spawned!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# ep7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] | 2021-02-25 02:05:51 AM | analyzer.app_simple0 | APK killed\n",
      "[INFO] | 2021-02-25 02:05:51 AM | analyzer.app_simple0 | Spawning the current app\n",
      "[INFO] | 2021-02-25 02:05:52 AM | analyzer.app_simple0 | Apk spawned successfully!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state: (1, 400, 17, 20)\n",
      "target: (1, 1, 17, 20)\n",
      "action: (1, 14, 17, 20)\n",
      "tout_preference.shape=torch.Size([1, 16])\n",
      "tout_action.shape=torch.Size([1, 14, 16])\n",
      "# tout2_mm: tensor([-2.6470, -2.6342, -2.6242, -2.6300, -2.6365, -2.6383, -2.6331, -2.6454,\n",
      "        -2.6336, -2.6282, -2.6342, -2.6530, -2.6520, -2.6578],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "# selected_action_id (mul): 0, log-sim: -2.6469926834106445\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/joseph/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:74: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state: (1, 400, 17, 20)\n",
      "target: (1, 1, 17, 20)\n",
      "action: (1, 14, 17, 20)\n",
      "tout_preference.shape=torch.Size([1, 16])\n",
      "tout_action.shape=torch.Size([1, 14, 16])\n",
      "# tout2_mm: tensor([-2.6440, -2.6286, -2.6249, -2.6362, -2.6342, -2.6377, -2.6355, -2.6419,\n",
      "        -2.6353, -2.6314, -2.6383, -2.6518, -2.6509, -2.6567],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "# selected_action_id (mul): 13, log-sim: -2.656658411026001\n",
      "state: (1, 400, 17, 20)\n",
      "target: (1, 1, 17, 20)\n",
      "action: (1, 14, 17, 20)\n",
      "tout_preference.shape=torch.Size([1, 16])\n",
      "tout_action.shape=torch.Size([1, 14, 16])\n",
      "# tout2_mm: tensor([-2.6440, -2.6286, -2.6249, -2.6362, -2.6342, -2.6377, -2.6355, -2.6419,\n",
      "        -2.6353, -2.6314, -2.6383, -2.6518, -2.6509, -2.6567],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "# selected_action_id (mul): 10, log-sim: -2.6382594108581543\n",
      "state: (1, 400, 17, 20)\n",
      "target: (1, 1, 17, 20)\n",
      "action: (1, 14, 17, 20)\n",
      "tout_preference.shape=torch.Size([1, 16])\n",
      "tout_action.shape=torch.Size([1, 14, 16])\n",
      "# tout2_mm: tensor([-2.6470, -2.6342, -2.6242, -2.6300, -2.6365, -2.6383, -2.6331, -2.6454,\n",
      "        -2.6336, -2.6282, -2.6342, -2.6530, -2.6520, -2.6578],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "# selected_action_id (mul): 3, log-sim: -2.630033016204834\n",
      "state: (1, 400, 17, 20)\n",
      "target: (1, 1, 17, 20)\n",
      "action: (1, 14, 17, 20)\n",
      "tout_preference.shape=torch.Size([1, 16])\n",
      "tout_action.shape=torch.Size([1, 14, 16])\n",
      "# tout2_mm: tensor([-2.6470, -2.6342, -2.6242, -2.6300, -2.6365, -2.6383, -2.6331, -2.6454,\n",
      "        -2.6336, -2.6282, -2.6342, -2.6530, -2.6520, -2.6578],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "# selected_action_id (mul): 4, log-sim: -2.6365344524383545\n",
      "# final reward: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] | 2021-02-25 02:06:13 AM | analyzer.app_simple0 | Kill the current app if already spawned!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# ep8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] | 2021-02-25 02:06:14 AM | analyzer.app_simple0 | APK killed\n",
      "[INFO] | 2021-02-25 02:06:14 AM | analyzer.app_simple0 | Spawning the current app\n",
      "[INFO] | 2021-02-25 02:06:14 AM | analyzer.app_simple0 | Apk spawned successfully!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state: (1, 400, 17, 20)\n",
      "target: (1, 1, 17, 20)\n",
      "action: (1, 14, 17, 20)\n",
      "tout_preference.shape=torch.Size([1, 16])\n",
      "tout_action.shape=torch.Size([1, 14, 16])\n",
      "# tout2_mm: tensor([-2.6470, -2.6342, -2.6242, -2.6300, -2.6365, -2.6383, -2.6331, -2.6454,\n",
      "        -2.6336, -2.6282, -2.6342, -2.6530, -2.6520, -2.6578],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "# selected_action_id (mul): 5, log-sim: -2.638345718383789\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/joseph/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:74: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state: (1, 400, 17, 20)\n",
      "target: (1, 1, 17, 20)\n",
      "action: (1, 14, 17, 20)\n",
      "tout_preference.shape=torch.Size([1, 16])\n",
      "tout_action.shape=torch.Size([1, 14, 16])\n",
      "# tout2_mm: tensor([-2.6470, -2.6342, -2.6242, -2.6300, -2.6365, -2.6383, -2.6331, -2.6454,\n",
      "        -2.6336, -2.6282, -2.6342, -2.6530, -2.6520, -2.6578],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "# selected_action_id (mul): 10, log-sim: -2.634199619293213\n",
      "state: (1, 400, 17, 20)\n",
      "target: (1, 1, 17, 20)\n",
      "action: (1, 14, 17, 20)\n",
      "tout_preference.shape=torch.Size([1, 16])\n",
      "tout_action.shape=torch.Size([1, 14, 16])\n",
      "# tout2_mm: tensor([-2.6470, -2.6342, -2.6242, -2.6300, -2.6365, -2.6383, -2.6331, -2.6454,\n",
      "        -2.6336, -2.6282, -2.6342, -2.6530, -2.6520, -2.6578],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "# selected_action_id (mul): 8, log-sim: -2.633577346801758\n",
      "state: (1, 400, 17, 20)\n",
      "target: (1, 1, 17, 20)\n",
      "action: (1, 14, 17, 20)\n",
      "tout_preference.shape=torch.Size([1, 16])\n",
      "tout_action.shape=torch.Size([1, 14, 16])\n",
      "# tout2_mm: tensor([-2.6470, -2.6342, -2.6242, -2.6300, -2.6365, -2.6383, -2.6331, -2.6454,\n",
      "        -2.6336, -2.6282, -2.6342, -2.6530, -2.6520, -2.6578],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "# selected_action_id (rnd): 4, log-sim: -2.6365344524383545\n",
      "state: (1, 400, 17, 20)\n",
      "target: (1, 1, 17, 20)\n",
      "action: (1, 14, 17, 20)\n",
      "tout_preference.shape=torch.Size([1, 16])\n",
      "tout_action.shape=torch.Size([1, 14, 16])\n",
      "# tout2_mm: tensor([-2.6470, -2.6342, -2.6242, -2.6300, -2.6365, -2.6383, -2.6331, -2.6454,\n",
      "        -2.6336, -2.6282, -2.6342, -2.6530, -2.6520, -2.6578],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "# selected_action_id (rnd): 12, log-sim: -2.651980400085449\n",
      "# final reward: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] | 2021-02-25 02:06:35 AM | analyzer.app_simple0 | Kill the current app if already spawned!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# ep9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] | 2021-02-25 02:06:35 AM | analyzer.app_simple0 | APK killed\n",
      "[INFO] | 2021-02-25 02:06:35 AM | analyzer.app_simple0 | Spawning the current app\n",
      "[INFO] | 2021-02-25 02:06:36 AM | analyzer.app_simple0 | Apk spawned successfully!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state: (1, 400, 17, 20)\n",
      "target: (1, 1, 17, 20)\n",
      "action: (1, 14, 17, 20)\n",
      "tout_preference.shape=torch.Size([1, 16])\n",
      "tout_action.shape=torch.Size([1, 14, 16])\n",
      "# tout2_mm: tensor([-2.6470, -2.6342, -2.6242, -2.6300, -2.6365, -2.6383, -2.6331, -2.6454,\n",
      "        -2.6336, -2.6282, -2.6342, -2.6530, -2.6520, -2.6578],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "# selected_action_id (mul): 5, log-sim: -2.638345956802368\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/joseph/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:74: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state: (1, 400, 17, 20)\n",
      "target: (1, 1, 17, 20)\n",
      "action: (1, 14, 17, 20)\n",
      "tout_preference.shape=torch.Size([1, 16])\n",
      "tout_action.shape=torch.Size([1, 14, 16])\n",
      "# tout2_mm: tensor([-2.6470, -2.6342, -2.6242, -2.6300, -2.6365, -2.6383, -2.6331, -2.6454,\n",
      "        -2.6336, -2.6282, -2.6342, -2.6530, -2.6520, -2.6578],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "# selected_action_id (mul): 12, log-sim: -2.651980400085449\n",
      "state: (1, 400, 17, 20)\n",
      "target: (1, 1, 17, 20)\n",
      "action: (1, 10, 17, 20)\n",
      "tout_preference.shape=torch.Size([1, 16])\n",
      "tout_action.shape=torch.Size([1, 10, 16])\n",
      "# tout2_mm: tensor([-2.3055, -2.2955, -2.2991, -2.3048, -2.2995, -2.2942, -2.2977, -2.3086,\n",
      "        -2.3076, -2.3134], grad_fn=<LogSoftmaxBackward>)\n",
      "# selected_action_id (mul): 4, log-sim: -2.2994799613952637\n",
      "state: (1, 400, 17, 20)\n",
      "target: (1, 1, 17, 20)\n",
      "action: (1, 10, 17, 20)\n",
      "tout_preference.shape=torch.Size([1, 16])\n",
      "tout_action.shape=torch.Size([1, 10, 16])\n",
      "# tout2_mm: tensor([-2.3055, -2.2955, -2.2991, -2.3048, -2.2995, -2.2942, -2.2977, -2.3086,\n",
      "        -2.3076, -2.3134], grad_fn=<LogSoftmaxBackward>)\n",
      "# selected_action_id (mul): 8, log-sim: -2.307648181915283\n",
      "state: (1, 400, 17, 20)\n",
      "target: (1, 1, 17, 20)\n",
      "action: (1, 10, 17, 20)\n",
      "tout_preference.shape=torch.Size([1, 16])\n",
      "tout_action.shape=torch.Size([1, 10, 16])\n",
      "# tout2_mm: tensor([-2.3055, -2.2955, -2.2991, -2.3048, -2.2995, -2.2942, -2.2977, -2.3086,\n",
      "        -2.3076, -2.3134], grad_fn=<LogSoftmaxBackward>)\n",
      "# selected_action_id (mul): 5, log-sim: -2.2941551208496094\n",
      "# final reward: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] | 2021-02-25 02:06:56 AM | analyzer.app_simple0 | Kill the current app if already spawned!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# ep10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] | 2021-02-25 02:06:56 AM | analyzer.app_simple0 | APK killed\n",
      "[INFO] | 2021-02-25 02:06:56 AM | analyzer.app_simple0 | Spawning the current app\n",
      "[INFO] | 2021-02-25 02:06:57 AM | analyzer.app_simple0 | Apk spawned successfully!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state: (1, 400, 17, 20)\n",
      "target: (1, 1, 17, 20)\n",
      "action: (1, 14, 17, 20)\n",
      "tout_preference.shape=torch.Size([1, 16])\n",
      "tout_action.shape=torch.Size([1, 14, 16])\n",
      "# tout2_mm: tensor([-2.6470, -2.6342, -2.6242, -2.6300, -2.6365, -2.6383, -2.6331, -2.6454,\n",
      "        -2.6336, -2.6282, -2.6342, -2.6530, -2.6520, -2.6578],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "# selected_action_id (mul): 9, log-sim: -2.628235101699829\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/joseph/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:74: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state: (1, 400, 17, 20)\n",
      "target: (1, 1, 17, 20)\n",
      "action: (1, 14, 17, 20)\n",
      "tout_preference.shape=torch.Size([1, 16])\n",
      "tout_action.shape=torch.Size([1, 14, 16])\n",
      "# tout2_mm: tensor([-2.6470, -2.6342, -2.6242, -2.6300, -2.6365, -2.6383, -2.6331, -2.6454,\n",
      "        -2.6336, -2.6282, -2.6342, -2.6530, -2.6520, -2.6578],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "# selected_action_id (mul): 5, log-sim: -2.638336181640625\n",
      "state: (1, 400, 17, 20)\n",
      "target: (1, 1, 17, 20)\n",
      "action: (1, 14, 17, 20)\n",
      "tout_preference.shape=torch.Size([1, 16])\n",
      "tout_action.shape=torch.Size([1, 14, 16])\n",
      "# tout2_mm: tensor([-2.6470, -2.6342, -2.6242, -2.6300, -2.6365, -2.6383, -2.6331, -2.6454,\n",
      "        -2.6336, -2.6282, -2.6342, -2.6530, -2.6520, -2.6578],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "# selected_action_id (rnd): 7, log-sim: -2.6453659534454346\n",
      "state: (1, 400, 17, 20)\n",
      "target: (1, 1, 17, 20)\n",
      "action: (1, 14, 17, 20)\n",
      "tout_preference.shape=torch.Size([1, 16])\n",
      "tout_action.shape=torch.Size([1, 14, 16])\n",
      "# tout2_mm: tensor([-2.6470, -2.6342, -2.6242, -2.6300, -2.6365, -2.6383, -2.6331, -2.6454,\n",
      "        -2.6336, -2.6282, -2.6342, -2.6530, -2.6520, -2.6578],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "# selected_action_id (rnd): 7, log-sim: -2.6453659534454346\n",
      "state: (1, 400, 17, 20)\n",
      "target: (1, 1, 17, 20)\n",
      "action: (1, 14, 17, 20)\n",
      "tout_preference.shape=torch.Size([1, 16])\n",
      "tout_action.shape=torch.Size([1, 14, 16])\n",
      "# tout2_mm: tensor([-2.6470, -2.6342, -2.6242, -2.6300, -2.6365, -2.6383, -2.6331, -2.6454,\n",
      "        -2.6336, -2.6282, -2.6342, -2.6530, -2.6520, -2.6578],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "# selected_action_id (rnd): 2, log-sim: -2.624239444732666\n",
      "# final reward: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] | 2021-02-25 02:07:18 AM | analyzer.app_simple0 | Kill the current app if already spawned!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# ep11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] | 2021-02-25 02:07:18 AM | analyzer.app_simple0 | APK killed\n",
      "[INFO] | 2021-02-25 02:07:18 AM | analyzer.app_simple0 | Spawning the current app\n",
      "[INFO] | 2021-02-25 02:07:19 AM | analyzer.app_simple0 | Apk spawned successfully!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state: (1, 400, 17, 20)\n",
      "target: (1, 1, 17, 20)\n",
      "action: (1, 14, 17, 20)\n",
      "tout_preference.shape=torch.Size([1, 16])\n",
      "tout_action.shape=torch.Size([1, 14, 16])\n",
      "# tout2_mm: tensor([-2.6470, -2.6342, -2.6242, -2.6300, -2.6365, -2.6383, -2.6331, -2.6454,\n",
      "        -2.6336, -2.6282, -2.6342, -2.6530, -2.6520, -2.6578],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "# selected_action_id (mul): 3, log-sim: -2.6300313472747803\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/joseph/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:74: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state: (1, 400, 17, 20)\n",
      "target: (1, 1, 17, 20)\n",
      "action: (1, 14, 17, 20)\n",
      "tout_preference.shape=torch.Size([1, 16])\n",
      "tout_action.shape=torch.Size([1, 14, 16])\n",
      "# tout2_mm: tensor([-2.6470, -2.6342, -2.6242, -2.6300, -2.6365, -2.6383, -2.6331, -2.6454,\n",
      "        -2.6336, -2.6282, -2.6342, -2.6530, -2.6520, -2.6578],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "# selected_action_id (mul): 12, log-sim: -2.6519927978515625\n",
      "state: (1, 400, 17, 20)\n",
      "target: (1, 1, 17, 20)\n",
      "action: (1, 10, 17, 20)\n",
      "tout_preference.shape=torch.Size([1, 16])\n",
      "tout_action.shape=torch.Size([1, 10, 16])\n",
      "# tout2_mm: tensor([-2.3055, -2.2955, -2.2990, -2.3048, -2.2995, -2.2941, -2.2977, -2.3086,\n",
      "        -2.3077, -2.3135], grad_fn=<LogSoftmaxBackward>)\n",
      "# selected_action_id (rnd): 1, log-sim: -2.295522451400757\n",
      "state: (1, 400, 17, 20)\n",
      "target: (1, 1, 17, 20)\n",
      "action: (1, 1, 17, 20)\n",
      "tout_preference.shape=torch.Size([1, 16])\n",
      "tout_action.shape=torch.Size([1, 1, 16])\n",
      "# tout2_mm: tensor([0.], grad_fn=<LogSoftmaxBackward>)\n",
      "# selected_action_id (mul): 0, log-sim: 0.0\n",
      "state: (1, 400, 17, 20)\n",
      "target: (1, 1, 17, 20)\n",
      "action: (1, 10, 17, 20)\n",
      "tout_preference.shape=torch.Size([1, 16])\n",
      "tout_action.shape=torch.Size([1, 10, 16])\n",
      "# tout2_mm: tensor([-2.3055, -2.2955, -2.2990, -2.3048, -2.2995, -2.2941, -2.2977, -2.3086,\n",
      "        -2.3077, -2.3135], grad_fn=<LogSoftmaxBackward>)\n",
      "# selected_action_id (mul): 2, log-sim: -2.2990481853485107\n",
      "# final reward: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] | 2021-02-25 02:07:39 AM | analyzer.app_simple0 | Kill the current app if already spawned!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# ep12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] | 2021-02-25 02:07:39 AM | analyzer.app_simple0 | APK killed\n",
      "[INFO] | 2021-02-25 02:07:39 AM | analyzer.app_simple0 | Spawning the current app\n",
      "[INFO] | 2021-02-25 02:07:40 AM | analyzer.app_simple0 | Apk spawned successfully!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state: (1, 400, 17, 20)\n",
      "target: (1, 1, 17, 20)\n",
      "action: (1, 14, 17, 20)\n",
      "tout_preference.shape=torch.Size([1, 16])\n",
      "tout_action.shape=torch.Size([1, 14, 16])\n",
      "# tout2_mm: tensor([-2.6470, -2.6342, -2.6242, -2.6300, -2.6365, -2.6383, -2.6331, -2.6454,\n",
      "        -2.6336, -2.6282, -2.6342, -2.6530, -2.6520, -2.6578],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "# selected_action_id (rnd): 0, log-sim: -2.6469931602478027\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/joseph/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:74: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state: (1, 400, 17, 20)\n",
      "target: (1, 1, 17, 20)\n",
      "action: (1, 14, 17, 20)\n",
      "tout_preference.shape=torch.Size([1, 16])\n",
      "tout_action.shape=torch.Size([1, 14, 16])\n",
      "# tout2_mm: tensor([-2.6440, -2.6286, -2.6249, -2.6362, -2.6342, -2.6377, -2.6355, -2.6419,\n",
      "        -2.6353, -2.6314, -2.6383, -2.6518, -2.6509, -2.6567],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "# selected_action_id (mul): 13, log-sim: -2.656672716140747\n",
      "state: (1, 400, 17, 20)\n",
      "target: (1, 1, 17, 20)\n",
      "action: (1, 14, 17, 20)\n",
      "tout_preference.shape=torch.Size([1, 16])\n",
      "tout_action.shape=torch.Size([1, 14, 16])\n",
      "# tout2_mm: tensor([-2.6440, -2.6286, -2.6249, -2.6362, -2.6342, -2.6377, -2.6355, -2.6419,\n",
      "        -2.6353, -2.6314, -2.6383, -2.6518, -2.6509, -2.6567],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "# selected_action_id (rnd): 6, log-sim: -2.635495662689209\n",
      "state: (1, 400, 17, 20)\n",
      "target: (1, 1, 17, 20)\n",
      "action: (1, 2, 17, 20)\n",
      "tout_preference.shape=torch.Size([1, 16])\n",
      "tout_action.shape=torch.Size([1, 2, 16])\n",
      "# tout2_mm: tensor([-0.6921, -0.6942], grad_fn=<LogSoftmaxBackward>)\n",
      "# selected_action_id (mul): 0, log-sim: -0.6921269297599792\n",
      "state: (1, 400, 17, 20)\n",
      "target: (1, 1, 17, 20)\n",
      "action: (1, 14, 17, 20)\n",
      "tout_preference.shape=torch.Size([1, 16])\n",
      "tout_action.shape=torch.Size([1, 14, 16])\n",
      "# tout2_mm: tensor([-2.6440, -2.6286, -2.6249, -2.6362, -2.6342, -2.6377, -2.6355, -2.6419,\n",
      "        -2.6353, -2.6314, -2.6383, -2.6518, -2.6509, -2.6567],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "# selected_action_id (mul): 13, log-sim: -2.656672716140747\n",
      "# final reward: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] | 2021-02-25 02:08:01 AM | analyzer.app_simple0 | Kill the current app if already spawned!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# ep13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] | 2021-02-25 02:08:01 AM | analyzer.app_simple0 | APK killed\n",
      "[INFO] | 2021-02-25 02:08:01 AM | analyzer.app_simple0 | Spawning the current app\n",
      "[INFO] | 2021-02-25 02:08:01 AM | analyzer.app_simple0 | Apk spawned successfully!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state: (1, 400, 17, 20)\n",
      "target: (1, 1, 17, 20)\n",
      "action: (1, 14, 17, 20)\n",
      "tout_preference.shape=torch.Size([1, 16])\n",
      "tout_action.shape=torch.Size([1, 14, 16])\n",
      "# tout2_mm: tensor([-2.6470, -2.6342, -2.6242, -2.6300, -2.6365, -2.6383, -2.6331, -2.6454,\n",
      "        -2.6336, -2.6282, -2.6342, -2.6530, -2.6520, -2.6578],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "# selected_action_id (mul): 8, log-sim: -2.633563995361328\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/joseph/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:74: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state: (1, 400, 17, 20)\n",
      "target: (1, 1, 17, 20)\n",
      "action: (1, 14, 17, 20)\n",
      "tout_preference.shape=torch.Size([1, 16])\n",
      "tout_action.shape=torch.Size([1, 14, 16])\n",
      "# tout2_mm: tensor([-2.6470, -2.6342, -2.6242, -2.6300, -2.6365, -2.6383, -2.6331, -2.6454,\n",
      "        -2.6336, -2.6282, -2.6342, -2.6530, -2.6520, -2.6578],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "# selected_action_id (mul): 13, log-sim: -2.6577963829040527\n",
      "state: (1, 400, 17, 20)\n",
      "target: (1, 1, 17, 20)\n",
      "action: (1, 14, 17, 20)\n",
      "tout_preference.shape=torch.Size([1, 16])\n",
      "tout_action.shape=torch.Size([1, 14, 16])\n",
      "# tout2_mm: tensor([-2.6470, -2.6342, -2.6242, -2.6300, -2.6365, -2.6383, -2.6331, -2.6454,\n",
      "        -2.6336, -2.6282, -2.6342, -2.6530, -2.6520, -2.6578],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "# selected_action_id (mul): 2, log-sim: -2.6242384910583496\n",
      "state: (1, 400, 17, 20)\n",
      "target: (1, 1, 17, 20)\n",
      "action: (1, 2, 17, 20)\n",
      "tout_preference.shape=torch.Size([1, 16])\n",
      "tout_action.shape=torch.Size([1, 2, 16])\n",
      "# tout2_mm: tensor([-0.6992, -0.6871], grad_fn=<LogSoftmaxBackward>)\n",
      "# selected_action_id (rnd): 0, log-sim: -0.6992443203926086\n",
      "state: (1, 400, 17, 20)\n",
      "target: (1, 1, 17, 20)\n",
      "action: (1, 14, 17, 20)\n",
      "tout_preference.shape=torch.Size([1, 16])\n",
      "tout_action.shape=torch.Size([1, 14, 16])\n",
      "# tout2_mm: tensor([-2.6470, -2.6342, -2.6242, -2.6300, -2.6365, -2.6383, -2.6331, -2.6454,\n",
      "        -2.6336, -2.6282, -2.6342, -2.6530, -2.6520, -2.6578],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "# selected_action_id (mul): 6, log-sim: -2.63308048248291\n",
      "# final reward: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] | 2021-02-25 02:08:21 AM | analyzer.app_simple0 | Kill the current app if already spawned!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# ep14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] | 2021-02-25 02:08:22 AM | analyzer.app_simple0 | APK killed\n",
      "[INFO] | 2021-02-25 02:08:22 AM | analyzer.app_simple0 | Spawning the current app\n",
      "[INFO] | 2021-02-25 02:08:22 AM | analyzer.app_simple0 | Apk spawned successfully!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state: (1, 400, 17, 20)\n",
      "target: (1, 1, 17, 20)\n",
      "action: (1, 14, 17, 20)\n",
      "tout_preference.shape=torch.Size([1, 16])\n",
      "tout_action.shape=torch.Size([1, 14, 16])\n",
      "# tout2_mm: tensor([-2.6470, -2.6342, -2.6242, -2.6300, -2.6365, -2.6383, -2.6331, -2.6454,\n",
      "        -2.6336, -2.6282, -2.6342, -2.6530, -2.6520, -2.6578],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "# selected_action_id (mul): 6, log-sim: -2.633080244064331\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/joseph/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:74: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state: (1, 400, 17, 20)\n",
      "target: (1, 1, 17, 20)\n",
      "action: (1, 2, 17, 20)\n",
      "tout_preference.shape=torch.Size([1, 16])\n",
      "tout_action.shape=torch.Size([1, 2, 16])\n",
      "# tout2_mm: tensor([-0.6864, -0.7000], grad_fn=<LogSoftmaxBackward>)\n",
      "# selected_action_id (rnd): 1, log-sim: -0.6999651789665222\n",
      "state: (1, 400, 17, 20)\n",
      "target: (1, 1, 17, 20)\n",
      "action: (1, 14, 17, 20)\n",
      "tout_preference.shape=torch.Size([1, 16])\n",
      "tout_action.shape=torch.Size([1, 14, 16])\n",
      "# tout2_mm: tensor([-2.6470, -2.6342, -2.6242, -2.6300, -2.6365, -2.6383, -2.6331, -2.6454,\n",
      "        -2.6336, -2.6282, -2.6342, -2.6530, -2.6520, -2.6578],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "# selected_action_id (mul): 5, log-sim: -2.638336181640625\n",
      "state: (1, 400, 17, 20)\n",
      "target: (1, 1, 17, 20)\n",
      "action: (1, 14, 17, 20)\n",
      "tout_preference.shape=torch.Size([1, 16])\n",
      "tout_action.shape=torch.Size([1, 14, 16])\n",
      "# tout2_mm: tensor([-2.6470, -2.6342, -2.6242, -2.6300, -2.6365, -2.6383, -2.6331, -2.6454,\n",
      "        -2.6336, -2.6282, -2.6342, -2.6530, -2.6520, -2.6578],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "# selected_action_id (mul): 4, log-sim: -2.636530876159668\n",
      "state: (1, 400, 17, 20)\n",
      "target: (1, 1, 17, 20)\n",
      "action: (1, 15, 17, 20)\n",
      "tout_preference.shape=torch.Size([1, 16])\n",
      "tout_action.shape=torch.Size([1, 15, 16])\n",
      "# tout2_mm: tensor([-2.7095, -2.7159, -2.7030, -2.6931, -2.6989, -2.7054, -2.7072, -2.7020,\n",
      "        -2.7143, -2.7025, -2.6971, -2.7031, -2.7219, -2.7209, -2.7267],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "# selected_action_id (mul): 4, log-sim: -2.6989190578460693\n",
      "# final reward: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] | 2021-02-25 02:08:43 AM | analyzer.app_simple0 | Kill the current app if already spawned!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# ep15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] | 2021-02-25 02:08:43 AM | analyzer.app_simple0 | APK killed\n",
      "[INFO] | 2021-02-25 02:08:43 AM | analyzer.app_simple0 | Spawning the current app\n",
      "[INFO] | 2021-02-25 02:08:44 AM | analyzer.app_simple0 | Apk spawned successfully!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state: (1, 400, 17, 20)\n",
      "target: (1, 1, 17, 20)\n",
      "action: (1, 14, 17, 20)\n",
      "tout_preference.shape=torch.Size([1, 16])\n",
      "tout_action.shape=torch.Size([1, 14, 16])\n",
      "# tout2_mm: tensor([-2.6470, -2.6342, -2.6242, -2.6300, -2.6365, -2.6383, -2.6331, -2.6454,\n",
      "        -2.6336, -2.6282, -2.6342, -2.6530, -2.6520, -2.6578],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "# selected_action_id (mul): 7, log-sim: -2.645366668701172\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/joseph/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:74: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state: (1, 400, 17, 20)\n",
      "target: (1, 1, 17, 20)\n",
      "action: (1, 14, 17, 20)\n",
      "tout_preference.shape=torch.Size([1, 16])\n",
      "tout_action.shape=torch.Size([1, 14, 16])\n",
      "# tout2_mm: tensor([-2.6470, -2.6342, -2.6242, -2.6300, -2.6365, -2.6383, -2.6331, -2.6454,\n",
      "        -2.6336, -2.6282, -2.6342, -2.6530, -2.6520, -2.6578],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "# selected_action_id (mul): 10, log-sim: -2.6341934204101562\n",
      "state: (1, 400, 17, 20)\n",
      "target: (1, 1, 17, 20)\n",
      "action: (1, 14, 17, 20)\n",
      "tout_preference.shape=torch.Size([1, 16])\n",
      "tout_action.shape=torch.Size([1, 14, 16])\n",
      "# tout2_mm: tensor([-2.6470, -2.6342, -2.6242, -2.6300, -2.6365, -2.6383, -2.6331, -2.6454,\n",
      "        -2.6336, -2.6282, -2.6342, -2.6530, -2.6520, -2.6578],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "# selected_action_id (mul): 9, log-sim: -2.6282331943511963\n",
      "state: (1, 400, 17, 20)\n",
      "target: (1, 1, 17, 20)\n",
      "action: (1, 15, 17, 20)\n",
      "tout_preference.shape=torch.Size([1, 16])\n",
      "tout_action.shape=torch.Size([1, 15, 16])\n",
      "# tout2_mm: tensor([-2.6931, -2.7171, -2.7042, -2.6943, -2.7001, -2.7066, -2.7084, -2.7032,\n",
      "        -2.7154, -2.7036, -2.6983, -2.7043, -2.7230, -2.7221, -2.7279],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "# selected_action_id (mul): 7, log-sim: -2.7031502723693848\n",
      "state: (1, 400, 17, 20)\n",
      "target: (1, 1, 17, 20)\n",
      "action: (1, 2, 17, 20)\n",
      "tout_preference.shape=torch.Size([1, 16])\n",
      "tout_action.shape=torch.Size([1, 2, 16])\n",
      "# tout2_mm: tensor([-0.6864, -0.7000], grad_fn=<LogSoftmaxBackward>)\n",
      "# selected_action_id (mul): 1, log-sim: -0.6999644637107849\n",
      "# final reward: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] | 2021-02-25 02:09:05 AM | analyzer.app_simple0 | Kill the current app if already spawned!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# ep16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] | 2021-02-25 02:09:05 AM | analyzer.app_simple0 | APK killed\n",
      "[INFO] | 2021-02-25 02:09:05 AM | analyzer.app_simple0 | Spawning the current app\n",
      "[INFO] | 2021-02-25 02:09:06 AM | analyzer.app_simple0 | Apk spawned successfully!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state: (1, 400, 17, 20)\n",
      "target: (1, 1, 17, 20)\n",
      "action: (1, 14, 17, 20)\n",
      "tout_preference.shape=torch.Size([1, 16])\n",
      "tout_action.shape=torch.Size([1, 14, 16])\n",
      "# tout2_mm: tensor([-2.6470, -2.6342, -2.6242, -2.6300, -2.6365, -2.6383, -2.6331, -2.6454,\n",
      "        -2.6336, -2.6282, -2.6342, -2.6530, -2.6520, -2.6578],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "# selected_action_id (mul): 0, log-sim: -2.6469931602478027"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/joseph/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:74: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "state: (1, 400, 17, 20)\n",
      "target: (1, 1, 17, 20)\n",
      "action: (1, 14, 17, 20)\n",
      "tout_preference.shape=torch.Size([1, 16])\n",
      "tout_action.shape=torch.Size([1, 14, 16])\n",
      "# tout2_mm: tensor([-2.6440, -2.6286, -2.6249, -2.6362, -2.6342, -2.6377, -2.6355, -2.6419,\n",
      "        -2.6353, -2.6314, -2.6383, -2.6518, -2.6509, -2.6567],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "# selected_action_id (rnd): 0, log-sim: -2.6439812183380127\n",
      "state: (1, 400, 17, 20)\n",
      "target: (1, 1, 17, 20)\n",
      "action: (1, 14, 17, 20)\n",
      "tout_preference.shape=torch.Size([1, 16])\n",
      "tout_action.shape=torch.Size([1, 14, 16])\n",
      "# tout2_mm: tensor([-2.6440, -2.6286, -2.6249, -2.6362, -2.6342, -2.6377, -2.6355, -2.6419,\n",
      "        -2.6353, -2.6314, -2.6383, -2.6518, -2.6509, -2.6567],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "# selected_action_id (mul): 8, log-sim: -2.635272741317749\n",
      "state: (1, 400, 17, 20)\n",
      "target: (1, 1, 17, 20)\n",
      "action: (1, 14, 17, 20)\n",
      "tout_preference.shape=torch.Size([1, 16])\n",
      "tout_action.shape=torch.Size([1, 14, 16])\n",
      "# tout2_mm: tensor([-2.6440, -2.6286, -2.6249, -2.6362, -2.6342, -2.6377, -2.6355, -2.6419,\n",
      "        -2.6353, -2.6314, -2.6383, -2.6518, -2.6509, -2.6567],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "# selected_action_id (mul): 5, log-sim: -2.637700319290161\n",
      "state: (1, 400, 17, 20)\n",
      "target: (1, 1, 17, 20)\n",
      "action: (1, 14, 17, 20)\n",
      "tout_preference.shape=torch.Size([1, 16])\n",
      "tout_action.shape=torch.Size([1, 14, 16])\n",
      "# tout2_mm: tensor([-2.6440, -2.6286, -2.6249, -2.6362, -2.6342, -2.6377, -2.6355, -2.6419,\n",
      "        -2.6353, -2.6314, -2.6383, -2.6518, -2.6509, -2.6567],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "# selected_action_id (mul): 7, log-sim: -2.6418964862823486\n",
      "# final reward: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] | 2021-02-25 02:09:27 AM | analyzer.app_simple0 | Kill the current app if already spawned!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# ep17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] | 2021-02-25 02:09:28 AM | analyzer.app_simple0 | APK killed\n",
      "[INFO] | 2021-02-25 02:09:28 AM | analyzer.app_simple0 | Spawning the current app\n",
      "[INFO] | 2021-02-25 02:09:28 AM | analyzer.app_simple0 | Apk spawned successfully!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state: (1, 400, 17, 20)\n",
      "target: (1, 1, 17, 20)\n",
      "action: (1, 14, 17, 20)\n",
      "tout_preference.shape=torch.Size([1, 16])\n",
      "tout_action.shape=torch.Size([1, 14, 16])\n",
      "# tout2_mm: tensor([-2.6470, -2.6342, -2.6242, -2.6300, -2.6365, -2.6383, -2.6331, -2.6454,\n",
      "        -2.6336, -2.6282, -2.6342, -2.6530, -2.6520, -2.6578],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "# selected_action_id (mul): 6, log-sim: -2.63308048248291\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/joseph/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:74: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state: (1, 400, 17, 20)\n",
      "target: (1, 1, 17, 20)\n",
      "action: (1, 2, 17, 20)\n",
      "tout_preference.shape=torch.Size([1, 16])\n",
      "tout_action.shape=torch.Size([1, 2, 16])\n",
      "# tout2_mm: tensor([-0.6864, -0.7000], grad_fn=<LogSoftmaxBackward>)\n",
      "# selected_action_id (mul): 1, log-sim: -0.6999649405479431\n",
      "state: (1, 400, 17, 20)\n",
      "target: (1, 1, 17, 20)\n",
      "action: (1, 14, 17, 20)\n",
      "tout_preference.shape=torch.Size([1, 16])\n",
      "tout_action.shape=torch.Size([1, 14, 16])\n",
      "# tout2_mm: tensor([-2.6470, -2.6342, -2.6242, -2.6300, -2.6365, -2.6383, -2.6331, -2.6454,\n",
      "        -2.6336, -2.6282, -2.6342, -2.6530, -2.6520, -2.6578],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "# selected_action_id (mul): 3, log-sim: -2.630030632019043\n",
      "state: (1, 400, 17, 20)\n",
      "target: (1, 1, 17, 20)\n",
      "action: (1, 14, 17, 20)\n",
      "tout_preference.shape=torch.Size([1, 16])\n",
      "tout_action.shape=torch.Size([1, 14, 16])\n",
      "# tout2_mm: tensor([-2.6470, -2.6342, -2.6242, -2.6300, -2.6365, -2.6383, -2.6331, -2.6454,\n",
      "        -2.6336, -2.6282, -2.6342, -2.6530, -2.6520, -2.6578],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "# selected_action_id (mul): 9, log-sim: -2.628234386444092\n",
      "state: (1, 400, 17, 20)\n",
      "target: (1, 1, 17, 20)\n",
      "action: (1, 14, 17, 20)\n",
      "tout_preference.shape=torch.Size([1, 16])\n",
      "tout_action.shape=torch.Size([1, 14, 16])\n",
      "# tout2_mm: tensor([-2.6470, -2.6342, -2.6242, -2.6300, -2.6365, -2.6383, -2.6331, -2.6454,\n",
      "        -2.6336, -2.6282, -2.6342, -2.6530, -2.6520, -2.6578],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "# selected_action_id (mul): 6, log-sim: -2.6330807209014893\n",
      "# final reward: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] | 2021-02-25 02:09:49 AM | analyzer.app_simple0 | Kill the current app if already spawned!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# ep18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] | 2021-02-25 02:09:50 AM | analyzer.app_simple0 | APK killed\n",
      "[INFO] | 2021-02-25 02:09:50 AM | analyzer.app_simple0 | Spawning the current app\n",
      "[INFO] | 2021-02-25 02:09:50 AM | analyzer.app_simple0 | Apk spawned successfully!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state: (1, 400, 17, 20)\n",
      "target: (1, 1, 17, 20)\n",
      "action: (1, 14, 17, 20)\n",
      "tout_preference.shape=torch.Size([1, 16])\n",
      "tout_action.shape=torch.Size([1, 14, 16])\n",
      "# tout2_mm: tensor([-2.6470, -2.6342, -2.6242, -2.6300, -2.6365, -2.6383, -2.6331, -2.6454,\n",
      "        -2.6336, -2.6282, -2.6342, -2.6530, -2.6520, -2.6578],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "# selected_action_id (mul): 1, log-sim: -2.6341562271118164\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/joseph/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:74: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state: (1, 400, 17, 20)\n",
      "target: (1, 1, 17, 20)\n",
      "action: (1, 14, 17, 20)\n",
      "tout_preference.shape=torch.Size([1, 16])\n",
      "tout_action.shape=torch.Size([1, 14, 16])\n",
      "# tout2_mm: tensor([-2.6470, -2.6342, -2.6242, -2.6300, -2.6365, -2.6383, -2.6331, -2.6454,\n",
      "        -2.6336, -2.6282, -2.6342, -2.6530, -2.6520, -2.6578],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "# selected_action_id (rnd): 10, log-sim: -2.6341946125030518\n",
      "state: (1, 400, 17, 20)\n",
      "target: (1, 1, 17, 20)\n",
      "action: (1, 14, 17, 20)\n",
      "tout_preference.shape=torch.Size([1, 16])\n",
      "tout_action.shape=torch.Size([1, 14, 16])\n",
      "# tout2_mm: tensor([-2.6470, -2.6342, -2.6242, -2.6300, -2.6365, -2.6383, -2.6331, -2.6454,\n",
      "        -2.6336, -2.6282, -2.6342, -2.6530, -2.6520, -2.6578],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "# selected_action_id (rnd): 5, log-sim: -2.638336658477783\n",
      "state: (1, 400, 17, 20)\n",
      "target: (1, 1, 17, 20)\n",
      "action: (1, 14, 17, 20)\n",
      "tout_preference.shape=torch.Size([1, 16])\n",
      "tout_action.shape=torch.Size([1, 14, 16])\n",
      "# tout2_mm: tensor([-2.6470, -2.6342, -2.6242, -2.6300, -2.6365, -2.6383, -2.6331, -2.6454,\n",
      "        -2.6336, -2.6282, -2.6342, -2.6530, -2.6520, -2.6578],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "# selected_action_id (mul): 3, log-sim: -2.630030393600464\n",
      "state: (1, 400, 17, 20)\n",
      "target: (1, 1, 17, 20)\n",
      "action: (1, 14, 17, 20)\n",
      "tout_preference.shape=torch.Size([1, 16])\n",
      "tout_action.shape=torch.Size([1, 14, 16])\n",
      "# tout2_mm: tensor([-2.6470, -2.6342, -2.6242, -2.6300, -2.6365, -2.6383, -2.6331, -2.6454,\n",
      "        -2.6336, -2.6282, -2.6342, -2.6530, -2.6520, -2.6578],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "# selected_action_id (mul): 13, log-sim: -2.657794952392578\n",
      "# final reward: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] | 2021-02-25 02:10:12 AM | analyzer.app_simple0 | Kill the current app if already spawned!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# ep19\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] | 2021-02-25 02:10:12 AM | analyzer.app_simple0 | APK killed\n",
      "[INFO] | 2021-02-25 02:10:12 AM | analyzer.app_simple0 | Spawning the current app\n",
      "[INFO] | 2021-02-25 02:10:13 AM | analyzer.app_simple0 | Apk spawned successfully!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state: (1, 400, 17, 20)\n",
      "target: (1, 1, 17, 20)\n",
      "action: (1, 14, 17, 20)\n",
      "tout_preference.shape=torch.Size([1, 16])\n",
      "tout_action.shape=torch.Size([1, 14, 16])\n",
      "# tout2_mm: tensor([-2.6470, -2.6342, -2.6242, -2.6300, -2.6365, -2.6383, -2.6331, -2.6454,\n",
      "        -2.6336, -2.6282, -2.6342, -2.6530, -2.6520, -2.6578],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "# selected_action_id (mul): 10, log-sim: -2.6341941356658936\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/joseph/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:74: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state: (1, 400, 17, 20)\n",
      "target: (1, 1, 17, 20)\n",
      "action: (1, 14, 17, 20)\n",
      "tout_preference.shape=torch.Size([1, 16])\n",
      "tout_action.shape=torch.Size([1, 14, 16])\n",
      "# tout2_mm: tensor([-2.6470, -2.6342, -2.6242, -2.6300, -2.6365, -2.6383, -2.6331, -2.6454,\n",
      "        -2.6336, -2.6282, -2.6342, -2.6530, -2.6520, -2.6578],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "# selected_action_id (rnd): 5, log-sim: -2.638336181640625\n",
      "state: (1, 400, 17, 20)\n",
      "target: (1, 1, 17, 20)\n",
      "action: (1, 14, 17, 20)\n",
      "tout_preference.shape=torch.Size([1, 16])\n",
      "tout_action.shape=torch.Size([1, 14, 16])\n",
      "# tout2_mm: tensor([-2.6470, -2.6342, -2.6242, -2.6300, -2.6365, -2.6383, -2.6331, -2.6454,\n",
      "        -2.6336, -2.6282, -2.6342, -2.6530, -2.6520, -2.6578],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "# selected_action_id (mul): 11, log-sim: -2.6529648303985596\n",
      "state: (1, 400, 17, 20)\n",
      "target: (1, 1, 17, 20)\n",
      "action: (1, 11, 17, 20)\n",
      "tout_preference.shape=torch.Size([1, 16])\n",
      "tout_action.shape=torch.Size([1, 11, 16])\n",
      "# tout2_mm: tensor([-2.4020, -2.3906, -2.3886, -2.3960, -2.4041, -2.3958, -2.3856, -2.3928,\n",
      "        -2.4060, -2.4050, -2.4108], grad_fn=<LogSoftmaxBackward>)\n",
      "# selected_action_id (mul): 2, log-sim: -2.3886144161224365\n",
      "state: (1, 400, 17, 20)\n",
      "target: (1, 1, 17, 20)\n",
      "action: (1, 3, 17, 20)\n",
      "tout_preference.shape=torch.Size([1, 16])\n",
      "tout_action.shape=torch.Size([1, 3, 16])\n",
      "# tout2_mm: tensor([-1.0984, -1.0987, -1.0987], grad_fn=<LogSoftmaxBackward>)\n",
      "# selected_action_id (rnd): 0, log-sim: -1.098402738571167\n",
      "# final reward: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] | 2021-02-25 02:10:34 AM | analyzer.app_simple0 | Kill the current app if already spawned!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# ep20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] | 2021-02-25 02:10:34 AM | analyzer.app_simple0 | APK killed\n",
      "[INFO] | 2021-02-25 02:10:34 AM | analyzer.app_simple0 | Spawning the current app\n",
      "[INFO] | 2021-02-25 02:10:35 AM | analyzer.app_simple0 | Apk spawned successfully!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state: (1, 400, 17, 20)\n",
      "target: (1, 1, 17, 20)\n",
      "action: (1, 14, 17, 20)\n",
      "tout_preference.shape=torch.Size([1, 16])\n",
      "tout_action.shape=torch.Size([1, 14, 16])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/joseph/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:74: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# tout2_mm: tensor([-2.6470, -2.6342, -2.6242, -2.6300, -2.6365, -2.6383, -2.6331, -2.6454,\n",
      "        -2.6336, -2.6282, -2.6342, -2.6530, -2.6520, -2.6578],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "# selected_action_id (mul): 9, log-sim: -2.6282339096069336\n",
      "state: (1, 400, 17, 20)\n",
      "target: (1, 1, 17, 20)\n",
      "action: (1, 14, 17, 20)\n",
      "tout_preference.shape=torch.Size([1, 16])\n",
      "tout_action.shape=torch.Size([1, 14, 16])\n",
      "# tout2_mm: tensor([-2.6470, -2.6342, -2.6242, -2.6300, -2.6365, -2.6383, -2.6331, -2.6454,\n",
      "        -2.6336, -2.6282, -2.6342, -2.6530, -2.6520, -2.6578],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "# selected_action_id (mul): 9, log-sim: -2.6282339096069336\n",
      "state: (1, 400, 17, 20)\n",
      "target: (1, 1, 17, 20)\n",
      "action: (1, 14, 17, 20)\n",
      "tout_preference.shape=torch.Size([1, 16])\n",
      "tout_action.shape=torch.Size([1, 14, 16])\n",
      "# tout2_mm: tensor([-2.6470, -2.6342, -2.6242, -2.6300, -2.6365, -2.6383, -2.6331, -2.6454,\n",
      "        -2.6336, -2.6282, -2.6342, -2.6530, -2.6520, -2.6578],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "# selected_action_id (mul): 7, log-sim: -2.6453664302825928\n",
      "state: (1, 400, 17, 20)\n",
      "target: (1, 1, 17, 20)\n",
      "action: (1, 14, 17, 20)\n",
      "tout_preference.shape=torch.Size([1, 16])\n",
      "tout_action.shape=torch.Size([1, 14, 16])\n",
      "# tout2_mm: tensor([-2.6470, -2.6342, -2.6242, -2.6300, -2.6365, -2.6383, -2.6331, -2.6454,\n",
      "        -2.6336, -2.6282, -2.6342, -2.6530, -2.6520, -2.6578],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "# selected_action_id (rnd): 4, log-sim: -2.636531352996826\n",
      "state: (1, 400, 17, 20)\n",
      "target: (1, 1, 17, 20)\n",
      "action: (1, 14, 17, 20)\n",
      "tout_preference.shape=torch.Size([1, 16])\n",
      "tout_action.shape=torch.Size([1, 14, 16])\n",
      "# tout2_mm: tensor([-2.6470, -2.6342, -2.6242, -2.6300, -2.6365, -2.6383, -2.6331, -2.6454,\n",
      "        -2.6336, -2.6282, -2.6342, -2.6530, -2.6520, -2.6578],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "# selected_action_id (mul): 9, log-sim: -2.6282341480255127\n",
      "# final reward: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] | 2021-02-25 02:11:00 AM | analyzer.app_simple0 | Kill the current app if already spawned!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# ep21\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] | 2021-02-25 02:11:00 AM | analyzer.app_simple0 | APK killed\n",
      "[INFO] | 2021-02-25 02:11:00 AM | analyzer.app_simple0 | Spawning the current app\n",
      "[INFO] | 2021-02-25 02:11:00 AM | analyzer.app_simple0 | Apk spawned successfully!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state: (1, 400, 17, 20)\n",
      "target: (1, 1, 17, 20)\n",
      "action: (1, 14, 17, 20)\n",
      "tout_preference.shape=torch.Size([1, 16])\n",
      "tout_action.shape=torch.Size([1, 14, 16])\n",
      "# tout2_mm: tensor([-2.6470, -2.6342, -2.6242, -2.6300, -2.6365, -2.6383, -2.6331, -2.6454,\n",
      "        -2.6336, -2.6282, -2.6342, -2.6530, -2.6520, -2.6578],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "# selected_action_id (mul): 1, log-sim: -2.634158134460449\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/joseph/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:74: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state: (1, 400, 17, 20)\n",
      "target: (1, 1, 17, 20)\n",
      "action: (1, 14, 17, 20)\n",
      "tout_preference.shape=torch.Size([1, 16])\n",
      "tout_action.shape=torch.Size([1, 14, 16])\n",
      "# tout2_mm: tensor([-2.6470, -2.6342, -2.6242, -2.6300, -2.6365, -2.6383, -2.6331, -2.6454,\n",
      "        -2.6336, -2.6282, -2.6342, -2.6530, -2.6520, -2.6578],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "# selected_action_id (mul): 4, log-sim: -2.636533498764038\n",
      "state: (1, 400, 17, 20)\n",
      "target: (1, 1, 17, 20)\n",
      "action: (1, 14, 17, 20)\n",
      "tout_preference.shape=torch.Size([1, 16])\n",
      "tout_action.shape=torch.Size([1, 14, 16])\n",
      "# tout2_mm: tensor([-2.6470, -2.6342, -2.6242, -2.6300, -2.6365, -2.6383, -2.6331, -2.6454,\n",
      "        -2.6336, -2.6282, -2.6342, -2.6530, -2.6520, -2.6578],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "# selected_action_id (mul): 1, log-sim: -2.634158134460449\n",
      "state: (1, 400, 17, 20)\n",
      "target: (1, 1, 17, 20)\n",
      "action: (1, 14, 17, 20)\n",
      "tout_preference.shape=torch.Size([1, 16])\n",
      "tout_action.shape=torch.Size([1, 14, 16])\n",
      "# tout2_mm: tensor([-2.6470, -2.6342, -2.6242, -2.6300, -2.6365, -2.6383, -2.6331, -2.6454,\n",
      "        -2.6336, -2.6282, -2.6342, -2.6530, -2.6520, -2.6578],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "# selected_action_id (rnd): 6, log-sim: -2.6330807209014893\n",
      "state: (1, 400, 17, 20)\n",
      "target: (1, 1, 17, 20)\n",
      "action: (1, 2, 17, 20)\n",
      "tout_preference.shape=torch.Size([1, 16])\n",
      "tout_action.shape=torch.Size([1, 2, 16])\n",
      "# tout2_mm: tensor([-0.6864, -0.7000], grad_fn=<LogSoftmaxBackward>)\n",
      "# selected_action_id (mul): 1, log-sim: -0.6999649405479431\n",
      "# final reward: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] | 2021-02-25 02:11:22 AM | analyzer.app_simple0 | Kill the current app if already spawned!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# ep22\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] | 2021-02-25 02:11:22 AM | analyzer.app_simple0 | APK killed\n",
      "[INFO] | 2021-02-25 02:11:22 AM | analyzer.app_simple0 | Spawning the current app\n",
      "[INFO] | 2021-02-25 02:11:23 AM | analyzer.app_simple0 | Apk spawned successfully!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state: (1, 400, 17, 20)\n",
      "target: (1, 1, 17, 20)\n",
      "action: (1, 14, 17, 20)\n",
      "tout_preference.shape=torch.Size([1, 16])\n",
      "tout_action.shape=torch.Size([1, 14, 16])\n",
      "# tout2_mm: tensor([-2.6470, -2.6342, -2.6242, -2.6300, -2.6365, -2.6383, -2.6331, -2.6454,\n",
      "        -2.6336, -2.6282, -2.6342, -2.6530, -2.6520, -2.6578],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "# selected_action_id (rnd): 13, log-sim: -2.6577913761138916\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/joseph/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:74: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state: (1, 400, 17, 20)\n",
      "target: (1, 1, 17, 20)\n",
      "action: (1, 14, 17, 20)\n",
      "tout_preference.shape=torch.Size([1, 16])\n",
      "tout_action.shape=torch.Size([1, 14, 16])\n",
      "# tout2_mm: tensor([-2.6470, -2.6342, -2.6242, -2.6300, -2.6365, -2.6383, -2.6331, -2.6454,\n",
      "        -2.6336, -2.6282, -2.6342, -2.6530, -2.6520, -2.6578],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "# selected_action_id (mul): 3, log-sim: -2.630033254623413\n",
      "state: (1, 400, 17, 20)\n",
      "target: (1, 1, 17, 20)\n",
      "action: (1, 14, 17, 20)\n",
      "tout_preference.shape=torch.Size([1, 16])\n",
      "tout_action.shape=torch.Size([1, 14, 16])\n",
      "# tout2_mm: tensor([-2.6470, -2.6342, -2.6242, -2.6300, -2.6365, -2.6383, -2.6331, -2.6454,\n",
      "        -2.6336, -2.6282, -2.6342, -2.6530, -2.6520, -2.6578],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "# selected_action_id (mul): 0, log-sim: -2.6469919681549072\n",
      "state: (1, 400, 17, 20)\n",
      "target: (1, 1, 17, 20)\n",
      "action: (1, 14, 17, 20)\n",
      "tout_preference.shape=torch.Size([1, 16])\n",
      "tout_action.shape=torch.Size([1, 14, 16])\n",
      "# tout2_mm: tensor([-2.6440, -2.6286, -2.6249, -2.6362, -2.6342, -2.6377, -2.6355, -2.6419,\n",
      "        -2.6353, -2.6314, -2.6383, -2.6518, -2.6509, -2.6567],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "# selected_action_id (mul): 0, log-sim: -2.6439807415008545\n"
     ]
    }
   ],
   "source": [
    "nsteps = 5\n",
    "neural_agent = NeuralAgent(EMBEDDING_DIM).to(device)\n",
    "optimizer = torch.optim.SGD(neural_agent.parameters(), lr=0.1)\n",
    "target = [\"com\",\"github\",\"cetoolbox\",\"CapillaryElectrophoresis\",\":\",\"double\",\"getMicroEOF()\"]\n",
    "target += [[\"\"]] * ( len(NODE_KEY_LIST)-len(target) )\n",
    "for ep in range(100):\n",
    "    print(\"# ep{}\".format(ep))\n",
    "    rollout(apk, neural_agent, optimizer, nsteps, target)\n",
    "#     random_rollout(apk_obj, nsteps, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<com.example.priyanka.testapp.BasicViewsActivity: void goToDisplay(android.view.View)> : 9',\n",
       " '<com.example.priyanka.testapp.BasicViewsActivity: void goToDisplay(android.view.View)> : 12']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "apk.get_reached_goal_states(\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apk.get_current_state()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<gui_elements.GuiElements at 0x7fb88dbfd0b8>,\n",
       " <gui_elements.GuiElements at 0x7fb88dbfd128>,\n",
       " <gui_elements.GuiElements at 0x7fb88dbfd208>,\n",
       " <gui_elements.GuiElements at 0x7fb88dbfd160>]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "apk.get_available_actionable_elements(apk.get_current_state())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apk.get_wtg_state()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
